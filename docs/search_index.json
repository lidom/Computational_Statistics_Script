[
["index.html", "Computational Statistics (B.Sc. Data Science) Informationen", " Computational Statistics (B.Sc. Data Science) Prof.¬†Dr.¬†Dominik Liebl Informationen Dies ist das Skript zur Vorlesung Computational Statistik (B.Sc. Data Science) Vorlesungszeiten Wochentag Uhrzeit H√∂rsaal Dienstag 9:15-10:45 Online-Vorlesung Freitag 8:30-10:00 Online-Vorlesung RCodes Die RCodes zu den einzelnen Kapiteln k√∂nnen hier heruntergeladen werden: LINK Leseecke Folgende frei zug√§ngliche Lehrb√ºcher enthalten Teile dieses Kurses. In den jeweiligen Kapiteln, werde ich auf die einzelnen B√ºcher verweisen. Pattern Recognition and Machine Learning (by Christopher Bishop) An Introduction to Statistical Learning, with Applications in R (by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani). Statistical Learning with Sparsity: the Lasso and Generalizations (by Trevor Hastie, Robert Tibshirani and Martin Wainwright). Elements of Statistical Learning: Data mining, Inference and Prediction (by Trevor Hastie, Robert Tibshirani and Jerome Friedman). Computer Age Statistical Inference: Algorithms, Evidence and Data Science (by Bradley Efron and Trevor Hastie) Florence Nightingale Das Logo zu diesem Skript stammt von einer Briefmarke zur Erinnerung an Florence Nightingale eine britische Krankenschwester und inspirierende Statistikerin. Dieses Skript ist lizenziert unter der Creative Commons Lizenz Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["1-der-expectation-maximization-em-algorithmus.html", "1 Der Expectation Maximization (EM) Algorithmus", " 1 Der Expectation Maximization (EM) Algorithmus span { display: inline-block; } Der EM Algorithmus wird h√§ufig verwendet, um komplizierte Maximum Likelihood Sch√§tz-Probleme zu vereinfachen. Wir stellen den Algorithmus zur Sch√§tzung von Gau√üschen Mischverteilungen (GMV) vor, da der EM-Algorithmus hier wohl seine h√§ufigste Anwendung hat. Bereits die originale Arbeit zum EM-Algorithmus (Dempster, Laird, und Rubin 1977) besch√§ftigt sich mit solchen Mischverteilungen. M√∂gliche Anwendungen von Gau√üschen Mischverteilungen: Automatisierte Videobearbeitungen: Z.B. Bildeinteilungen in Vorder- und Hintergrund. (Hier w√ºrde man jede Pixel-Farbkodierung mit Hilfe einer Gau√üschen Mischverteilungen modellieren.) Automatisierte Erkennung von Laufstilen Generell: Auffinden von Gruppierungen (zwei oder mehr) in den Daten (Clusteranalyse). Lernziele f√ºr dieses Kapitel Sie k√∂nnen ‚Ä¶ ein Anwendungsfeld des EM-Algorithmuses benennen. die Probleme der klassischen Maximum Likelihood Methode zur Sch√§tzung von Gau√üschen Mischverteilungen benennen und erkl√§utern. die Grundidee des EM-Algorithmuses erl√§utern. den EM-Algorithmus zur Sch√§tzung von Gau√üschen Mischverteilungen anwenden. das Grundidee der Vervollst√§ndigung der Daten durch latente Variablen erl√§utern. Begleitlekt√ºre(n) Zur Vorbereitung der Klausur ist es grunds√§tzlich aussreichend das Kursskript durchzuarbeiten - aber Lesen hat ja noch nie geschadet. Dieses Kapitel basiert haupts√§chlich auf: Kapitel 9 in Pattern Recognition and Machine Learning (Bishop 2006). (Der Link f√ºhrt zur frei erh√§ltlichen pdf-Version des Buches.) Weiterer guter Lesestoff zum EM Algoithmus gibt es z.B. hier: Kapitel 8.5 in Elements of Statistical Learning: Data Mining, Inference and Prediction (Hastie, Tibshirani, und Friedman 2009). (Der Link f√ºhrt zur frei erh√§ltlichen pdf-Version des Buches.) R-Pakete f√ºr diese Kapitel Installiere die notwendige R-Pakete f√ºr dieses Kapitel: pkgs &lt;- c(&quot;tidyverse&quot;, # Die tidyverse-Pakete &quot;palmerpenguins&quot;, # Pinguin-Daten &quot;scales&quot;, # F√ºr transparente Farben: alpha() &quot;RColorBrewer&quot;, # H√ºbsche Farben &quot;mclust&quot;) # Sch√§tzung und Verwendung von GMVs install.packages(pkgs) Literatur "],
["1-1-motivation-clusteranalyse-mit-hilfe-gau√üscher-mischverteilungen.html", "1.1 Motivation: Clusteranalyse mit Hilfe Gau√üscher Mischverteilungen", " 1.1 Motivation: Clusteranalyse mit Hilfe Gau√üscher Mischverteilungen Der folgende Code-Chunck bereitet die Daten auf. Achtung: Wir haben zwar die Information zu den verschiedenen Pinguin-Arten (Penguine_Art) tun aber im Folgenden so, also ob wir diese Information nicht kennen w√ºrden. Wir wollen alleine auf Basis der Flossenl√§ngen (Penguine_Flosse) die Gruppenzugeh√∂rigkeiten per Clusteranalyse bestimmen. (Im Nachhinein k√∂nnen wir dann mit Hilfe der Daten in Penguine_Art pr√ºfen, wie gut unsere Clusteranalyse ist.) library(&quot;palmerpenguins&quot;) # Pinguin-Daten library(&quot;scales&quot;) # F√ºr transparente Farben: scales::alpha() library(&quot;RColorBrewer&quot;) # H√ºbsche Farben col_v &lt;- RColorBrewer::brewer.pal(n = 3, name = &quot;Set2&quot;) ## Vorbereitung der Daten: Pinguine &lt;- palmerpenguins::penguins %&gt;% # Pinguin-Daten tidyr::as_tibble() %&gt;% # Datenformat: &#39;tibble&#39;-dataframe dplyr::filter(species!=&quot;Adelie&quot;) %&gt;% # Pinguin-Art &#39;Adelie&#39; l√∂schen (verbleiben: &#39;Chinstrap&#39; und &#39;Gentoo&#39;) droplevels() %&gt;% # L√∂sche das nicht mehr ben√∂tigte Adelie-Level tidyr::drop_na() %&gt;% # NAs l√∂schen dplyr::mutate(Art = species, # Variablen umbenennen Flosse = flipper_length_mm) %&gt;% dplyr::select(Art, Flosse) # Variablen ausw√§hlen ## n &lt;- nrow(Pinguine) # Stichprobenumfang ## Variable &#39;Penguine_Art&#39; aus Pinguine-Daten herausziehen Penguine_Art &lt;- dplyr::pull(Pinguine, Art) ## Variable &#39;Penguine_Flosse&#39; aus Pinguine-Daten herausziehen Penguine_Flosse &lt;- dplyr::pull(Pinguine, Flosse) ## Plot ## Histogramm: hist(x = Penguine_Flosse, freq = FALSE, xlab=&quot;Flosse (mm)&quot;, main=&quot;Pinguine\\n(Zwei Gruppen)&quot;, col=gray(.65,.5), border=gray(.35,.5), ylim=c(0.0003, 0.039)) ## Stipchart hinzuf√ºgen: stripchart(x = Penguine_Flosse, method = &quot;jitter&quot;, jitter = .0005, at = .001, pch = 21, col=alpha(col_v[3],.5), bg=alpha(col_v[3],.5), cex=1.3, add = TRUE) Das Clusterverfahren basierend auf Gau√üschen Mischverteilungen: Gau√üsche Mischverteilung (per EM-Algorithmus) sch√§tzen Die Datenpunkte wie bei der Linearen (oder Quadratischen) Diskriminanz-Analyse den Gruppen zuordnen (siehe Abbildung 1.1) Abbildung 1.1: Clusteranalyse basierend auf einer Mischverteilung mit zwei gewichteten Normalverteilungen. Abbildung 1.1 zeigt das Resultat einer Clusteranalyse basierend auf einer Mischverteilung zweier gewichteter Normalverteilungen. Cluster-Ergebnis: 95% der Pinguine konnten richtig zugeordnet werden - lediglich auf Basis ihrer Flossenl√§ngen. Mit Hilfe der folgenden R-Codes kann die obige Clusteranalyse und die Ergebnisgrafik repliziert werden: ## Clusteranalyse mit Hilfe von Gau√üschen Mischmodellen: mclust R-Paket: suppressMessages(library(&quot;mclust&quot;)) ## Anzahl der Gruppen G &lt;- 2 ## Sch√§tzung der Gau√üschen Mischverteilung mit Hilfe des EM Algorithmuses ## (inkl. Clusteranalyse) mclust_obj &lt;- mclust::Mclust(data = Penguine_Flosse, G=G, modelNames = &quot;V&quot;, verbose = FALSE) # summary(mclust_obj) # str(mclust_obj) ## Gesch√§tzte Gruppen-Zuordnungen class &lt;- mclust_obj$classification ## Anteil der korrekten Zuordnungen: # cbind(class, Penguine_Art) round(sum(class == as.numeric(Penguine_Art))/n, 2) ## Gesch√§tzte Mittelwerte mean_m &lt;- t(mclust_obj$parameters$mean) ## Gesch√§tzte Varianzen (und evtl. Kovarianzen) cov_l &lt;- list(&quot;Cov1&quot; = mclust_obj$parameters$variance$sigmasq[1], &quot;Cov2&quot; = mclust_obj$parameters$variance$sigmasq[2]) ## Gesch√§tzte Gewichte (a-priori-Wahrscheinlichkeiten) prop_v &lt;- mclust_obj$parameters$pro ## Auswerten der Gau√üsche Mischungs-Dichtefunktion np &lt;- 100 # Anzahl der Auswertungspunkte xxd &lt;- seq(min(Penguine_Flosse)-3, max(Penguine_Flosse)+5, length.out = np) ## Mischungs-Dichte yyd &lt;- dnorm(xxd, mean_m[1], sqrt(cov_l[[1]]))*prop_v[1] + dnorm(xxd, mean_m[2], sqrt(cov_l[[2]]))*prop_v[2] ## Einzel-Dichten yyd1 &lt;- dnorm(xxd, mean_m[1], sqrt(cov_l[[1]]))*prop_v[1] yyd2 &lt;- dnorm(xxd, mean_m[2], sqrt(cov_l[[2]]))*prop_v[2] ## Plot hist(x = Penguine_Flosse, xlab=&quot;Flosse (mm)&quot;, main=&quot;Pinguine\\n(Zwei Gruppen)&quot;, col=gray(.65,.5), border=gray(.35,.5), freq = FALSE, ylim=c(0, 0.04)) lines(x = xxd, y=yyd, lwd=2, col=gray(.35,.75)) lines(x = xxd, y=yyd1, lwd=2, col=gray(.35,.75), lty=2) lines(x = xxd, y=yyd2, lwd=2, col=gray(.35,.75), lty=2) stripchart(Penguine_Flosse[class==1], method = &quot;jitter&quot;, jitter = .0005, at = .001, pch = 21, col=alpha(col_v[1],.5), bg=alpha(col_v[1],.5), cex=1.3, add = TRUE) stripchart(Penguine_Flosse[class==2], method = &quot;jitter&quot;, jitter = .0005, at = .001, pch = 21, col=alpha(col_v[2],.5), bg=alpha(col_v[2],.5), cex=1.3, add = TRUE) "],
["1-2-der-em-algorithmus-zur-ml-sch√§tzung-gau√üscher-mischverteilungen.html", "1.2 Der EM Algorithmus zur ML-Sch√§tzung Gau√üscher Mischverteilungen", " 1.2 Der EM Algorithmus zur ML-Sch√§tzung Gau√üscher Mischverteilungen 1.2.1 Gau√üsche Mischmodelle (GMM) Eine Zufallsvariable \\(X\\), die einer Gauschen Mischverteilung folgt, bezeichnen wir als \\[ X\\sim\\mathcal{N}_{\\mathcal{mix}}(G,\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}) \\] Die dazugeh√∂rige Dichtefunktion einer Gau√üschen Mischverteilung ist folgenderma√üen definiert: \\[\\begin{equation} f_G(x|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})=\\sum_{g=1}^G\\pi_gf(x|\\mu_g\\sigma_g) \\tag{1.1} \\end{equation}\\] Gewichte: \\(\\boldsymbol{\\pi}=(\\pi_1,\\dots,\\pi_G)\\) mit \\(\\pi_g&gt;0\\) und \\(\\sum_{g=1}^G\\pi_g=1\\) Mittelwerte: \\(\\boldsymbol{\\mu}=(\\mu_1,\\dots,\\mu_G)\\) mit \\(\\mu_g\\in\\mathbb{R}\\) Standardabweichungen: \\(\\boldsymbol{\\sigma}=(\\sigma_1,\\dots,\\sigma_G)\\) mit \\(\\sigma_g&gt;0\\) Normalverteilung der Gruppe \\(g=1,\\dots,G\\): \\[ f(x|\\mu_g\\sigma_g)=\\frac{1}{\\sqrt{2\\pi}\\sigma_g}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu_g}{\\sigma_g}\\right)^2\\right) \\] Unbekannte Parameter: \\(\\boldsymbol{\\pi}\\), \\(\\boldsymbol{\\mu}\\) und \\(\\boldsymbol{\\sigma}\\) 1.2.2 Maximum Likelihood (ML) Sch√§tzung Man kann versuchen die unbekannten Parameter \\(\\boldsymbol{\\pi}=(\\pi_1,\\dots,\\pi_G)\\), \\(\\boldsymbol{\\mu}=(\\mu_1,\\dots,\\mu_G)\\) und \\(\\boldsymbol{\\sigma}=(\\sigma_1,\\dots,\\sigma_G)\\) eines Gau√üschen Mischmodells klassisch mit Hilfe der Maximum Likelihood Methode zu sch√§tzen. Ich sag‚Äôs gleich: Der Versuch wird scheitern. Wiederholung der Grundidee der ML-Sch√§tzung: Annahme: Die Daten \\(\\mathbf{x}=(x_1,\\dots,x_n)\\) sind eine Realisation einer einfachen (also i.i.d.) Zufallsstichprobe \\((X_1,\\dots,X_n)\\) mit \\[ X_i\\sim\\mathcal{N}_{\\mathcal{mix}}(G,\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}) \\] f√ºr alle \\(i=1,\\dots,n\\). Die Daten \\(\\mathbf{x}=(x_1,\\dots,x_n)\\) ‚Äûkennen‚Äú also die unbekannten Parameter \\(\\boldsymbol{\\pi},\\) \\(\\boldsymbol{\\mu}\\) und \\(\\boldsymbol{\\sigma}\\) und wir m√ºssen ihnen diese Informationen ‚Äûnur noch‚Äú entlocken. Sch√§tz-Idee: W√§hle \\(\\boldsymbol{\\pi}\\), \\(\\boldsymbol{\\mu}\\) und \\(\\boldsymbol{\\sigma}\\) so, dass \\(f_G(\\cdot|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\) ‚Äûoptimal‚Äú zu den beobachteten Daten \\(\\mathbf{x}\\) passt. Umsetzung der Sch√§tz-Idee: Maximiere (bzgl. \\(\\boldsymbol{\\pi}\\), \\(\\boldsymbol{\\mu}\\) und \\(\\boldsymbol{\\sigma}\\)) die Likelihood Funktion \\[\\mathcal{L}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x})=\\prod_{i=1}^nf_G(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\] Bzw. maximiere die Log-Likelihood Funktion (einfachere Maximierung) \\[\\begin{align*} \\ln\\left(\\mathcal{L}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x})\\right)= \\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x}) =&amp;\\sum_{i=1}^n\\ln\\left(f_G(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\right)\\\\ =&amp;\\sum_{i=1}^n\\ln\\left(\\sum_{g=1}^G\\pi_g\\phi_{\\mu_g\\sigma_g}(x_i)\\right) \\end{align*}\\] Beachte: Die Maximierung muss die Parameterrestriktionen in (1.1) ber√ºcksichtigen (\\(\\sigma_g&gt;0\\) und \\(\\pi_g&gt;0\\) f√ºr alle \\(g=1,\\dots,G\\) und \\(\\sum_{g=1}^G\\pi_g=1\\)). Die maximierenden Parameterwerte \\(\\hat{\\boldsymbol{\\pi}}\\), \\(\\hat{\\boldsymbol{\\mu}}\\) und \\(\\hat{\\boldsymbol{\\sigma}}\\) sind die ML-Sch√§tzer. Das kann man so ausdr√ºcken: \\[ (\\hat{\\boldsymbol{\\pi}},\\hat{\\boldsymbol{\\mu}},\\hat{\\boldsymbol{\\sigma}})=\\arg\\min_{\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}}\\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x}) \\] üòí Numerische L√∂sungen: Versucht man obiges Maximierungsproblem numerisch mit Hilfe des Computers zu l√∂sen, wird man schnell merken, dass die Ergebnisse h√∂chst instabil, unplausibel und wenig vertrauensw√ºrdig sind. F√ºr echte GMMs (\\(G&gt;1\\)) treten w√§hrend einer numerischen Maximierung sehr leicht Probleme mit Singularit√§ten auf. Dies geschieht immer dann, wenn eine der Normalverteilungskomponenten versucht den ganzen Datensatz \\(\\mathbf{x}\\) zu beschreiben und die andere(n) Normalverteilungskomponente(n) versuchen lediglich einzelne Datenpunkte zu beschreiben. Eine Gau√üsche Dichtefunktion \\(f_g\\), die sich um einen einzigen Datenpunkt \\(x_i\\) konzentriert (d.h. \\(\\mu_g=x_i\\) und \\(\\sigma_g\\to 0\\)) wird dabei sehr gro√üe Werte annehmen (d.h. \\(f_g(x_i)\\to\\infty\\)) und so die Log-Likelihood auf unerw√ºnschte maximieren. Solche trivialen Maximierungsl√∂sungen resultieren i.d.R. in unplausiblen Sch√§tzergebnissen. üòí Analytische L√∂sung: Es ist zwar etwas m√ºhsam, aber man kann versuchen die Log-Likelihood analytisch zu maximieren. Tut man dies sich das an, kommt man zu folgenden Ausdr√ºcken: \\[\\begin{align*} \\hat\\pi_g&amp;=\\frac{1}{n}\\sum_{i=1}^np_{ig}\\\\ \\hat\\mu_g&amp;=\\sum_{i=1}^n\\frac{p_{ig}}{\\left(\\sum_{j=1}^np_{jg}\\right)}x_i\\\\ \\hat\\sigma_g&amp;=\\sqrt{\\sum_{i=1}^n\\frac{p_{ig}}{\\left(\\sum_{j=1}^np_{jg}\\right)}\\left(x_i-\\hat\\mu_g\\right)^2} \\end{align*}\\] f√ºr \\(g=1,\\dots,G\\). Die Herleitung der Ausdr√ºcke f√ºr \\(\\mu_g\\), \\(\\sigma_g\\) und \\(\\pi_g\\), \\(g=1,\\dots,G\\), ist wirklich etwas l√§stig (mehrfache Anwendungen der Kettenregel, Produktregel, etc., sowie Anwendung des Lagrange-Multiplikator Verfahrens zur Optimierung unter Nebenbedingungen) aber machbar. In einer der √úbungsaufgaben d√ºrfen Sie den Ausdruck f√ºr \\(\\hat\\mu_g\\) herleiten. üôà Aber: Diese Ausdr√ºcke f√ºr \\(\\hat\\pi_g\\), \\(\\hat\\mu_g\\) und \\(\\hat\\sigma_g\\) h√§ngen von den unbekannten Parametern \\(\\boldsymbol{\\pi}=(\\pi_1,\\dots,\\pi_G)\\), \\(\\boldsymbol{\\mu}=(\\mu_1,\\dots,\\mu_G)\\) und \\(\\boldsymbol{\\sigma}=(\\sigma_1,\\dots,\\sigma_G)\\), denn: \\[ p_{ig}=\\frac{\\pi_g\\phi_{\\mu_g\\sigma_g}(x_i)}{f_G(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})} \\] f√ºr \\(i=1,\\dots,n\\) und \\(g=1,\\dots,G\\). Erlauben also keine direkte Sch√§tzung der unbekannten Parameter \\(\\boldsymbol{\\pi}\\), \\(\\boldsymbol{\\mu}\\) und \\(\\boldsymbol{\\sigma}\\) ü•≥ L√∂sung: Der EM Algorithmus 1.2.3 Der EM Algorithmus f√ºr GMMs Die Ausdr√ºcke f√ºr \\(\\hat\\pi_g\\), \\(\\hat\\mu_g\\) und \\(\\hat\\sigma_g\\) legen jedoch ein einfaches iteratives ML-Sch√§tzverfahren nahe: N√§mlich einer alternierenden Sch√§tzung von \\(p_{ig}\\) und \\(\\hat\\pi_g\\), \\(\\hat\\mu_g\\) und \\(\\hat\\sigma_g\\). Der Der EM Algorithmus: Setze Startwerte \\(\\boldsymbol{\\pi}^{(0)}\\), \\(\\boldsymbol{\\mu}^{(0)}\\) und \\(\\boldsymbol{\\sigma}^{(0)}\\) F√ºr \\(r=1,2,\\dots\\) (Expectation) Berechne: \\[p_{ig}^{(r)}=\\frac{\\pi_g^{(r-1)}\\phi_{\\mu^{(r-1)}_g\\sigma_g^{(r-1)}}(x_i)}{f_G(x_i|\\boldsymbol{\\pi}^{(r-1)},\\boldsymbol{\\mu}^{(r-1)},\\boldsymbol{\\sigma}^{(r-1)})}\\] (Maximization) Berechne: \\(\\hat\\pi_g^{(r)}=\\frac{1}{n}\\sum_{i=1}^np_{ig}^{(r)},\\quad\\quad\\hat\\mu_g^{(r)}=\\sum_{i=1}^n\\frac{p_{ig}^{(r)}}{\\left(\\sum_{j=1}^np_{jg}^{(r)}\\right)}x_i\\) \\(\\hat\\sigma_g^{(r)}=\\sqrt{\\sum_{i=1}^n\\frac{p_{ig}^{(r)}}{\\left(\\sum_{j=1}^np_{jg}^{(r)}\\right)}\\left(x_i-\\hat\\mu_g^{(r)}\\right)^2}\\) Pr√ºfe Konvergenz Der obige pseudo-Code wird im Folgenden Code-Chunck umgesetzt: library(&quot;MASS&quot;) library(&quot;mclust&quot;) ## Daten: x &lt;- cbind(Penguine_Flosse) # Daten [n x d]-Dimensional. d &lt;- ncol(x) # Dimension (d=1: univariat) n &lt;- nrow(x) # Stichprobenumfang G &lt;- 2 # Anzahl Gruppen ## Weitere Deklarationen: llk &lt;- matrix(NA, n, G) p &lt;- matrix(NA, n, G) loglikOld &lt;- 1e07 tol &lt;- 1e-05 it &lt;- 0 check &lt;- TRUE ## EM Algorithmus ## 1. Startwerte f√ºr pi, mu und sigma: pi &lt;- rep(1/G, G) # Naive pi sigma &lt;- array(diag(d), c(d,d,G)) # Varianz = 1 mu &lt;- t(MASS::mvrnorm(G, colMeans(x), sigma[,,1]*4) ) while(check){ ## 2.a Expectation-Schritt for(g in 1:G){ p[,g] &lt;- pi[g] * mclust:::dmvnorm(x, mu[,g], sigma[,,g]) } p &lt;- sweep(p, 1, STATS = rowSums(p), FUN = &quot;/&quot;) ## 2.b Maximization-Schritt par &lt;- mclust::covw(x, p, normalize = FALSE) mu &lt;- par$mean sigma &lt;- par$S pi &lt;- colMeans(p) ## 3. Pr√ºfung der Konvergenz for(g in 1:G) { llk[,g] &lt;- pi[g] * mclust:::dmvnorm(x, mu[,g], sigma[,,g]) } loglik &lt;- sum(log(rowSums(llk))) # aktueller Log-Likelihood Wert ## diff &lt;- abs(loglik - loglikOld)/abs(loglik) loglikOld &lt;- loglik it &lt;- it + 1 ## Anderung der Log-Likelihood noch gro√ü genug? check &lt;- diff &gt; tol } ## Sch√§tz-Resultate: results &lt;- matrix(c(pi, mu, sqrt(sigma)), nrow = 3, ncol = 2, byrow = TRUE, dimnames = list( c(&quot;Gewichte&quot;, &quot;Mittelwerte&quot;, &quot;Standardabweichungen&quot;), c(&quot;Gruppe 1&quot;, &quot;Gruppe 2&quot;))) ## results %&gt;% round(., 2) #&gt; Gruppe 1 Gruppe 2 #&gt; Gewichte 0.69 0.31 #&gt; Mittelwerte 216.20 194.27 #&gt; Standardabweichungen 7.31 6.27 Das Sch√§tzergebnis erlaubt es uns, Abbildung 1.1 zu replizieren: ## Auswerten der Gau√üsche Mischungs-Dichtefunktion np &lt;- 100 # Anzahl der Auswertungspunkte xxd &lt;- seq(min(Penguine_Flosse)-3, max(Penguine_Flosse)+5, length.out = np) ## Mischungs-Dichte yyd &lt;- dnorm(xxd, mu[1,1], sqrt(sigma)[,,1])*pi[1] + dnorm(xxd, mu[1,2], sqrt(sigma)[,,2])*pi[2] ## Einzel-Dichten yyd1 &lt;- dnorm(xxd, mu[1,1], sqrt(sigma)[,,1])*pi[1] yyd2 &lt;- dnorm(xxd, mu[1,2], sqrt(sigma)[,,2])*pi[2] ## Plot hist(x = Penguine_Flosse, xlab=&quot;Flosse (mm)&quot;, main=&quot;Pinguine\\n(Zwei Gruppen)&quot;, col=gray(.65,.5), border=gray(.35,.5), freq = FALSE, ylim=c(0, 0.04)) lines(x = xxd, y=yyd, lwd=2, col=gray(.35,.75)) lines(x = xxd, y=yyd1, lwd=2, col=gray(.35,.75), lty=2) lines(x = xxd, y=yyd2, lwd=2, col=gray(.35,.75), lty=2) abline(v=203.1, lty=3) stripchart(Penguine_Flosse[class==1], method = &quot;jitter&quot;, jitter = .0005, at = .001, pch = 21, col=alpha(col_v[1],.5), bg=alpha(col_v[1],.5), cex=1.3, add = TRUE) stripchart(Penguine_Flosse[class==2], method = &quot;jitter&quot;, jitter = .0005, at = .001, pch = 21, col=alpha(col_v[2],.5), bg=alpha(col_v[2],.5), cex=1.3, add = TRUE) "],
["1-3-der-alternative-wahre-blick-auf-den-em-algorithmus.html", "1.3 Der alternative (wahre) Blick auf den EM-Algorithmus", " 1.3 Der alternative (wahre) Blick auf den EM-Algorithmus Der EM Algorithmus erm√∂glicht es Maximum Likelihood Probleme zu vereinfachen, indem man die Daten durch nicht beobachtete (‚Äûlatente‚Äú) Variablen vervollst√§ndigt. Zur Erinnerung: Wie haben es ja nicht geschafft, die Log-Likelihood Funktion \\[ \\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x}) =\\sum_{i=1}^n\\ln\\left(\\sum_{g=1}^G\\pi_g\\phi_{\\mu_g\\sigma_g}(x_i)\\right) \\] direkt zu maximieren. Die \\(\\log(\\sum_{g=1}^G[\\dots])\\)-Konstruktion macht einem das Leben schwer. In unseren Pinguin-Daten gibt zwei Gruppen (\\(g\\in\\{1,2\\}\\)). Ist g√§be im Prinzip also latente (unbeobachtete) Zuordnungsdaten \\(z_{ig}\\) mit \\[ z_{ig}= \\left\\{\\begin{array}{ll} 1&amp;\\text{falls Pinguin }i\\text{ zu Gruppe }g\\text{ geh√∂rt.}\\\\ 0&amp;\\text{sonst.}\\\\ \\end{array}\\right., \\] wobei \\(\\sum_{g=1}^Gz_{ig}=1\\) f√ºr alle \\(i=1,\\dots,n\\). Beachte: F√ºr jeden Datenpunkt \\(i\\) (jeder Pinguin \\(i\\)) gibt es nur eine Gruppe (daher \\(\\sum_{g=1}^Gz_{ig}=1\\)). Dies ist eine wichtige Restriktion von GMMs, welche bei den Pinguin-Daten unproblematisch ist, in anderen Anwendungen aber evtl. problematisch sein kann. Die Zuordnungsdaten \\(\\mathbf{z}=(z_{11},\\dots,z_{nG})\\) sind leider unbekannt (latent). Wir wissen aber trotzdem etwas √ºber diese Zuordnungen. Laut unserem Modell \\[ f_G(x|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})=\\sum_{g=1}^G\\pi_gf(x|\\mu_g\\sigma_g), \\] ist die Zuordnung \\(z_{ig}\\) n√§mlich eine Realisation einer Bernoulli Zufallsvariable \\[ Z_{ig}\\sim\\mathcal{B}(\\pi_g) \\] Also \\[\\begin{align*} P(Z_{ig}=1)&amp;=\\;\\;\\;\\pi_g\\;\\;=P(\\text{Pinguin $i$ geh√∂rt zu Gruppe }g)\\\\ P(Z_{ig}=0)&amp;=1-\\pi_g=P(\\text{Pinguin $i$ geh√∂rt nicht zu Gruppe }g) \\end{align*}\\] Man bezeichnet \\(\\pi_1,\\dots,\\pi_G\\) als die ‚Äûa-priori-Wahrscheinlichkeiten‚Äú. Wenn wir nichts √ºber die Flossenl√§nge von Pinguin \\(i\\) wissen, dann bleiben uns nur die a-priori-Wahrscheinlichkeiten: Mit Wahrscheinlichkeit \\(\\pi_g\\) geh√∂rt Pinguin \\(i\\) zu Gruppe \\(g\\). Falls wir die Flossenl√§nge von Pinguin \\(i\\) erfahren, k√∂nnen wir die a-priori-Wahrscheinlichkeiten mit Hilfe des Satzes von Bayes aktualisieren. Dies f√ºhrt dann zur a-posteriori-Wahrscheinlichkeit: A-posteriori-Wahrscheinlichkeit \\(\\;p_{ig}\\): (Satz von Bayes) \\[\\begin{align*} p_{ig} &amp;=\\frac{\\pi_g\\phi_{\\mu_g\\sigma_g}(x_i)}{f_G(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}\\\\[2ex] &amp;=\\frac{\\overbrace{P(Z_{ig}=1)}^{\\text{‚ÄûA-priori-Wahrs.‚Äú}}\\phi_{\\mu_g\\sigma_g}(x_i)}{f_G(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}=\\overbrace{P(Z_{ig}=1|X_i=x_i)}^{\\text{‚ÄûA-posteriori-Wahrs.‚Äú}}=p_{ig}\\\\ \\end{align*}\\] Beachte: \\(p_{ig}\\) ist ein (bedingter) Mittelwert (Expectation) \\[\\begin{align*} p_{ig}&amp;=\\underbrace{1\\cdot P(Z_{ig}=1|X_i=x_i)+0\\cdot P(Z_{ig}=0|X_i=x_i)}_{=E(Z_{ig}=1|X_i=x_i)}\\\\ \\end{align*}\\] Der EM Algorithmus Setze Startwerte \\(\\boldsymbol{\\pi}^{(0)}\\), \\(\\boldsymbol{\\mu}^{(0)}\\) und \\(\\boldsymbol{\\sigma}^{(0)}\\) F√ºr \\(r=1,2,\\dots\\) (Expectation) Berechne: \\(p_{ig}^{(r)}=E\\left(Z_{ig}^{(r-1)}\\left|X_i^{(r-1)}=x_i\\right.\\right)\\) \\[\\begin{align*} \\text{wobei }\\;Z_{ig}^{(r-1)}&amp;\\sim\\mathcal{B}\\left(\\pi_g^{(r-1)}\\right)\\\\ \\text{und }\\;X_i^{(r-1)}&amp;\\sim \\mathcal{N}_{\\mathcal{mix}}(G,\\boldsymbol{\\pi}^{(r-1)},\\boldsymbol{\\mu}^{(r-1)},\\boldsymbol{\\sigma}^{(r-1)}) \\end{align*}\\] (Maximization) Berechne: \\(\\hat\\pi_g^{(r)}\\), \\(\\hat\\mu_g^{(r)}\\), \\(\\hat\\sigma_g^{(r)}\\) Pr√ºfe Konvergenz ‚Äì&gt; "],
["literatur.html", "Literatur", " Literatur "]
]
