[
["index.html", "Computational Statistics (B.Sc. Data Science) Informationen", " Computational Statistics (B.Sc. Data Science) Prof. Dr. Dominik Liebl Informationen Dies ist das Skript zur Vorlesung Computational Statistik (B.Sc. Data Science) Vorlesungszeiten Wochentag Uhrzeit Hörsaal Dienstag 9:15-10:45 Online-Vorlesung Freitag 8:30-10:00 Online-Vorlesung RCodes Die RCodes zu den einzelnen Kapiteln können hier heruntergeladen werden: LINK Leseecke Folgende frei zugängliche Lehrbücher enthalten Teile dieses Kurses. In den jeweiligen Kapiteln, werde ich auf die einzelnen Bücher verweisen. Pattern Recognition and Machine Learning (by Christopher Bishop) An Introduction to Statistical Learning, with Applications in R (by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani). Statistical Learning with Sparsity: the Lasso and Generalizations (by Trevor Hastie, Robert Tibshirani and Martin Wainwright). Elements of Statistical Learning: Data mining, Inference and Prediction (by Trevor Hastie, Robert Tibshirani and Jerome Friedman). Computer Age Statistical Inference: Algorithms, Evidence and Data Science (by Bradley Efron and Trevor Hastie) Florence Nightingale Das Logo zu diesem Skript stammt von einer Briefmarke zur Erinnerung an Florence Nightingale eine britische Krankenschwester und inspirierende Statistikerin. Dieses Skript ist lizenziert unter der Creative Commons Lizenz Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["1-der-expectation-maximization-em-algorithmus.html", "1 Der Expectation Maximization (EM) Algorithmus", " 1 Der Expectation Maximization (EM) Algorithmus span { display: inline-block; } Der EM Algorithmus wird häufig verwendet, um komplizierte Maximum Likelihood Schätz-Probleme zu vereinfachen. Wir stellen den Algorithmus zur Schätzung von Gaußschen Mischverteilungen (GMV) vor, da der EM-Algorithmus hier wohl seine häufigste Anwendung hat. Bereits die originale Arbeit zum EM-Algorithmus (Dempster, Laird, und Rubin 1977) beschäftigt sich mit solchen Mischverteilungen. Mögliche Anwendungen von Gaußschen Mischverteilungen: Automatisierte Videobearbeitungen: Z.B. Bildeinteilungen in Vorder- und Hintergrund. (Hier würde man jede Pixel-Farbkodierung mit Hilfe einer Gaußschen Mischverteilungen modellieren.) Automatisierte Erkennung von Laufstilen Generell: Auffinden von Gruppierungen (zwei oder mehr) in den Daten (Clusteranalyse). Lernziele für dieses Kapitel Sie können … ein Anwendungsfeld des EM-Algorithmuses benennen. die Probleme der klassischen Maximum Likelihood Methode zur Schätzung von Gaußschen Mischverteilungen benennen und erkläutern. die Grundidee des EM-Algorithmuses erläutern. den EM-Algorithmus zur Schätzung von Gaußschen Mischverteilungen anwenden. das Grundidee der Vervollständigung der Daten durch latente Variablen erläutern. Begleitlektüre(n) Zur Vorbereitung der Klausur ist es grundsätzlich aussreichend das Kursskript durchzuarbeiten - aber Lesen hat ja noch nie geschadet. Dieses Kapitel basiert hauptsächlich auf: Kapitel 9 in Pattern Recognition and Machine Learning (Bishop 2006). (Der Link führt zur frei erhältlichen pdf-Version des Buches.) Weiterer guter Lesestoff zum EM Algoithmus gibt es z.B. hier: Kapitel 8.5 in Elements of Statistical Learning: Data Mining, Inference and Prediction (Hastie, Tibshirani, und Friedman 2009). (Der Link führt zur frei erhältlichen pdf-Version des Buches.) R-Pakete für diese Kapitel Installiere die notwendige R-Pakete für dieses Kapitel: pkgs &lt;- c(&quot;tidyverse&quot;, # Die tidyverse-Pakete &quot;palmerpenguins&quot;, # Pinguin-Daten &quot;scales&quot;, # Für transparente Farben: alpha() &quot;RColorBrewer&quot;, # Hübsche Farben &quot;mclust&quot;) # Schätzung und Verwendung von GMVs install.packages(pkgs) Literatur "],
["1-1-motivation-clusteranalyse-mit-hilfe-gaußscher-mischverteilungen.html", "1.1 Motivation: Clusteranalyse mit Hilfe Gaußscher Mischverteilungen", " 1.1 Motivation: Clusteranalyse mit Hilfe Gaußscher Mischverteilungen Der folgende Code-Chunck bereitet die Daten auf. Achtung: Wir haben zwar die Information zu den verschiedenen Pinguin-Arten (Penguine_Art) tun aber im Folgenden so, also ob wir diese Information nicht kennen würden. Wir wollen alleine auf Basis der Flossenlängen (Penguine_Flosse) die Gruppenzugehörigkeiten per Clusteranalyse bestimmen. (Im Nachhinein können wir dann mit Hilfe der Daten in Penguine_Art prüfen, wie gut unsere Clusteranalyse ist.) library(&quot;palmerpenguins&quot;) # Pinguin-Daten library(&quot;scales&quot;) # Für transparente Farben: scales::alpha() library(&quot;RColorBrewer&quot;) # Hübsche Farben col_v &lt;- RColorBrewer::brewer.pal(n = 3, name = &quot;Set2&quot;) ## Vorbereitung der Daten: Pinguine &lt;- palmerpenguins::penguins %&gt;% # Pinguin-Daten tidyr::as_tibble() %&gt;% # Datenformat: &#39;tibble&#39;-dataframe dplyr::filter(species!=&quot;Adelie&quot;) %&gt;% # Pinguin-Art &#39;Adelie&#39; löschen (verbleiben: &#39;Chinstrap&#39; und &#39;Gentoo&#39;) droplevels() %&gt;% # Lösche das nicht mehr benötigte Adelie-Level tidyr::drop_na() %&gt;% # NAs löschen dplyr::mutate(Art = species, # Variablen umbenennen Flosse = flipper_length_mm) %&gt;% dplyr::select(Art, Flosse) # Variablen auswählen ## n &lt;- nrow(Pinguine) # Stichprobenumfang ## Variable &#39;Penguine_Art&#39; aus Pinguine-Daten herausziehen Penguine_Art &lt;- dplyr::pull(Pinguine, Art) ## Variable &#39;Penguine_Flosse&#39; aus Pinguine-Daten herausziehen Penguine_Flosse &lt;- dplyr::pull(Pinguine, Flosse) ## Plot ## Histogramm: hist(x = Penguine_Flosse, freq = FALSE, xlab=&quot;Flosse (mm)&quot;, main=&quot;Pinguine\\n(Zwei Gruppen)&quot;, col=gray(.65,.5), border=gray(.35,.5), ylim=c(0.0003, 0.039)) ## Stipchart hinzufügen: stripchart(x = Penguine_Flosse, method = &quot;jitter&quot;, jitter = .0005, at = .001, pch = 21, col=alpha(col_v[3],.5), bg=alpha(col_v[3],.5), cex=1.3, add = TRUE) Das Clusterverfahren basierend auf Gaußschen Mischverteilungen: Gaußsche Mischverteilung (per EM-Algorithmus) schätzen Die Datenpunkte wie bei der Linearen (oder Quadratischen) Diskriminanz-Analyse den Gruppen zuordnen (siehe Abbildung 1.1) Abbildung 1.1: Clusteranalyse basierend auf einer Mischverteilung mit zwei gewichteten Normalverteilungen. Abbildung 1.1 zeigt das Resultat einer Clusteranalyse basierend auf einer Mischverteilung zweier gewichteter Normalverteilungen. Cluster-Ergebnis: 95% der Pinguine konnten richtig zugeordnet werden - lediglich auf Basis ihrer Flossenlängen. Mit Hilfe der folgenden R-Codes kann die obige Clusteranalyse und die Ergebnisgrafik repliziert werden: ## Clusteranalyse mit Hilfe von Gaußschen Mischmodellen: mclust R-Paket: suppressMessages(library(&quot;mclust&quot;)) ## Anzahl der Gruppen G &lt;- 2 ## Schätzung der Gaußschen Mischverteilung mit Hilfe des EM Algorithmuses ## (inkl. Clusteranalyse) mclust_obj &lt;- mclust::Mclust(data = Penguine_Flosse, G=G, modelNames = &quot;V&quot;, verbose = FALSE) # summary(mclust_obj) # str(mclust_obj) ## Geschätzte Gruppen-Zuordnungen class &lt;- mclust_obj$classification ## Anteil der korrekten Zuordnungen: # cbind(class, Penguine_Art) round(sum(class == as.numeric(Penguine_Art))/n, 2) ## Geschätzte Mittelwerte mean_m &lt;- t(mclust_obj$parameters$mean) ## Geschätzte Varianzen (und evtl. Kovarianzen) cov_l &lt;- list(&quot;Cov1&quot; = mclust_obj$parameters$variance$sigmasq[1], &quot;Cov2&quot; = mclust_obj$parameters$variance$sigmasq[2]) ## Geschätzte Gewichte (a-priori-Wahrscheinlichkeiten) prop_v &lt;- mclust_obj$parameters$pro ## Auswerten der Gaußsche Mischungs-Dichtefunktion np &lt;- 100 # Anzahl der Auswertungspunkte xxd &lt;- seq(min(Penguine_Flosse)-3, max(Penguine_Flosse)+5, length.out = np) ## Mischungs-Dichte yyd &lt;- dnorm(xxd, mean_m[1], sqrt(cov_l[[1]]))*prop_v[1] + dnorm(xxd, mean_m[2], sqrt(cov_l[[2]]))*prop_v[2] ## Einzel-Dichten yyd1 &lt;- dnorm(xxd, mean_m[1], sqrt(cov_l[[1]]))*prop_v[1] yyd2 &lt;- dnorm(xxd, mean_m[2], sqrt(cov_l[[2]]))*prop_v[2] ## Plot hist(x = Penguine_Flosse, xlab=&quot;Flosse (mm)&quot;, main=&quot;Pinguine\\n(Zwei Gruppen)&quot;, col=gray(.65,.5), border=gray(.35,.5), freq = FALSE, ylim=c(0, 0.04)) lines(x = xxd, y=yyd, lwd=2, col=gray(.35,.75)) lines(x = xxd, y=yyd1, lwd=2, col=gray(.35,.75), lty=2) lines(x = xxd, y=yyd2, lwd=2, col=gray(.35,.75), lty=2) stripchart(Penguine_Flosse[class==1], method = &quot;jitter&quot;, jitter = .0005, at = .001, pch = 21, col=alpha(col_v[1],.5), bg=alpha(col_v[1],.5), cex=1.3, add = TRUE) stripchart(Penguine_Flosse[class==2], method = &quot;jitter&quot;, jitter = .0005, at = .001, pch = 21, col=alpha(col_v[2],.5), bg=alpha(col_v[2],.5), cex=1.3, add = TRUE) "],
["1-2-der-em-algorithmus-zur-ml-schätzung-gaußscher-mischverteilungen.html", "1.2 Der EM Algorithmus zur ML-Schätzung Gaußscher Mischverteilungen", " 1.2 Der EM Algorithmus zur ML-Schätzung Gaußscher Mischverteilungen 1.2.1 Gaußsche Mischmodelle (GMM) Eine Zufallsvariable \\(X\\), die einer Gauschen Mischverteilung folgt, bezeichnen wir als \\[ X\\sim\\mathcal{N}_{\\mathcal{mix}}(G,\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}) \\] Die dazugehörige Dichtefunktion einer Gaußschen Mischverteilung ist folgendermaßen definiert: \\[\\begin{equation} f_G(x|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})=\\sum_{g=1}^G\\pi_gf(x|\\mu_g\\sigma_g) \\tag{1.1} \\end{equation}\\] Gewichte: \\(\\boldsymbol{\\pi}=(\\pi_1,\\dots,\\pi_G)\\) mit \\(\\pi_g&gt;0\\) und \\(\\sum_{g=1}^G\\pi_g=1\\) Mittelwerte: \\(\\boldsymbol{\\mu}=(\\mu_1,\\dots,\\mu_G)\\) mit \\(\\mu_g\\in\\mathbb{R}\\) Standardabweichungen: \\(\\boldsymbol{\\sigma}=(\\sigma_1,\\dots,\\sigma_G)\\) mit \\(\\sigma_g&gt;0\\) Normalverteilung der Gruppe \\(g=1,\\dots,G\\): \\[ f(x|\\mu_g\\sigma_g)=\\frac{1}{\\sqrt{2\\pi}\\sigma_g}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu_g}{\\sigma_g}\\right)^2\\right) \\] Unbekannte Parameter: \\(\\boldsymbol{\\pi}\\), \\(\\boldsymbol{\\mu}\\) und \\(\\boldsymbol{\\sigma}\\) 1.2.2 Maximum Likelihood (ML) Schätzung Man kann versuchen die unbekannten Parameter \\(\\boldsymbol{\\pi}=(\\pi_1,\\dots,\\pi_G)\\), \\(\\boldsymbol{\\mu}=(\\mu_1,\\dots,\\mu_G)\\) und \\(\\boldsymbol{\\sigma}=(\\sigma_1,\\dots,\\sigma_G)\\) eines Gaußschen Mischmodells klassisch mit Hilfe der Maximum Likelihood Methode zu schätzen. Ich sag’s gleich: Der Versuch wird scheitern. Wiederholung der Grundidee der ML-Schätzung: Annahme: Die Daten \\(\\mathbf{x}=(x_1,\\dots,x_n)\\) sind eine Realisation einer einfachen (also i.i.d.) Zufallsstichprobe \\((X_1,\\dots,X_n)\\) mit \\[ X_i\\sim\\mathcal{N}_{\\mathcal{mix}}(G,\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}) \\] für alle \\(i=1,\\dots,n\\). Die Daten \\(\\mathbf{x}=(x_1,\\dots,x_n)\\) „kennen“ also die unbekannten Parameter \\(\\boldsymbol{\\pi},\\) \\(\\boldsymbol{\\mu}\\) und \\(\\boldsymbol{\\sigma}\\) und wir müssen ihnen diese Informationen „nur noch“ entlocken. Schätz-Idee: Wähle \\(\\boldsymbol{\\pi}\\), \\(\\boldsymbol{\\mu}\\) und \\(\\boldsymbol{\\sigma}\\) so, dass \\(f_G(\\cdot|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\) „optimal“ zu den beobachteten Daten \\(\\mathbf{x}\\) passt. Umsetzung der Schätz-Idee: Maximiere (bzgl. \\(\\boldsymbol{\\pi}\\), \\(\\boldsymbol{\\mu}\\) und \\(\\boldsymbol{\\sigma}\\)) die Likelihood Funktion \\[\\mathcal{L}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x})=\\prod_{i=1}^nf_G(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\] Bzw. maximiere die Log-Likelihood Funktion (einfachere Maximierung) \\[\\begin{align*} \\ln\\left(\\mathcal{L}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x})\\right)= \\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x}) =&amp;\\sum_{i=1}^n\\ln\\left(f_G(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\right)\\\\ =&amp;\\sum_{i=1}^n\\ln\\left(\\sum_{g=1}^G\\pi_g\\phi_{\\mu_g\\sigma_g}(x_i)\\right) \\end{align*}\\] Beachte: Die Maximierung muss die Parameterrestriktionen in (1.1) berücksichtigen (\\(\\sigma_g&gt;0\\) und \\(\\pi_g&gt;0\\) für alle \\(g=1,\\dots,G\\) und \\(\\sum_{g=1}^G\\pi_g=1\\)). Die maximierenden Parameterwerte \\(\\hat{\\boldsymbol{\\pi}}\\), \\(\\hat{\\boldsymbol{\\mu}}\\) und \\(\\hat{\\boldsymbol{\\sigma}}\\) sind die ML-Schätzer. Das kann man so ausdrücken: \\[ (\\hat{\\boldsymbol{\\pi}},\\hat{\\boldsymbol{\\mu}},\\hat{\\boldsymbol{\\sigma}})=\\arg\\min_{\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}}\\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x}) \\] 😒 Numerische Lösungen: Versucht man obiges Maximierungsproblem numerisch mit Hilfe des Computers zu lösen, wird man schnell merken, dass die Ergebnisse höchst instabil, unplausibel und wenig vertrauenswürdig sind. Für echte GMMs (\\(G&gt;1\\)) treten während einer numerischen Maximierung sehr leicht Probleme mit Singularitäten auf. Dies geschieht immer dann, wenn eine der Normalverteilungskomponenten versucht den ganzen Datensatz \\(\\mathbf{x}\\) zu beschreiben und die andere(n) Normalverteilungskomponente(n) versuchen lediglich einzelne Datenpunkte zu beschreiben. Eine Gaußsche Dichtefunktion \\(f_g\\), die sich um einen einzigen Datenpunkt \\(x_i\\) konzentriert (d.h. \\(\\mu_g=x_i\\) und \\(\\sigma_g\\to 0\\)) wird dabei sehr große Werte annehmen (d.h. \\(f_g(x_i)\\to\\infty\\)) und so die Log-Likelihood auf unerwünschte maximieren. Solche trivialen Maximierungslösungen resultieren i.d.R. in unplausiblen Schätzergebnissen. 😒 Analytische Lösung: Es ist zwar etwas mühsam, aber man kann versuchen die Log-Likelihood analytisch zu maximieren. Tut man dies sich das an, kommt man zu folgenden Ausdrücken: \\[\\begin{align*} \\hat\\pi_g&amp;=\\frac{1}{n}\\sum_{i=1}^np_{ig}\\\\ \\hat\\mu_g&amp;=\\sum_{i=1}^n\\frac{p_{ig}}{\\left(\\sum_{j=1}^np_{jg}\\right)}x_i\\\\ \\hat\\sigma_g&amp;=\\sqrt{\\sum_{i=1}^n\\frac{p_{ig}}{\\left(\\sum_{j=1}^np_{jg}\\right)}\\left(x_i-\\hat\\mu_g\\right)^2} \\end{align*}\\] für \\(g=1,\\dots,G\\). Die Herleitung der Ausdrücke für \\(\\mu_g\\), \\(\\sigma_g\\) und \\(\\pi_g\\), \\(g=1,\\dots,G\\), ist wirklich etwas lästig (mehrfache Anwendungen der Kettenregel, Produktregel, etc., sowie Anwendung des Lagrange-Multiplikator Verfahrens zur Optimierung unter Nebenbedingungen) aber machbar. In einer der Übungsaufgaben dürfen Sie den Ausdruck für \\(\\hat\\mu_g\\) herleiten. 🙈 Aber: Diese Ausdrücke für \\(\\hat\\pi_g\\), \\(\\hat\\mu_g\\) und \\(\\hat\\sigma_g\\) hängen von den unbekannten Parametern \\(\\boldsymbol{\\pi}=(\\pi_1,\\dots,\\pi_G)\\), \\(\\boldsymbol{\\mu}=(\\mu_1,\\dots,\\mu_G)\\) und \\(\\boldsymbol{\\sigma}=(\\sigma_1,\\dots,\\sigma_G)\\), denn: \\[ p_{ig}=\\frac{\\pi_g\\phi_{\\mu_g\\sigma_g}(x_i)}{f_G(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})} \\] für \\(i=1,\\dots,n\\) und \\(g=1,\\dots,G\\). Erlauben also keine direkte Schätzung der unbekannten Parameter \\(\\boldsymbol{\\pi}\\), \\(\\boldsymbol{\\mu}\\) und \\(\\boldsymbol{\\sigma}\\) 🥳 Lösung: Der EM Algorithmus 1.2.3 Der EM Algorithmus für GMMs Die Ausdrücke für \\(\\hat\\pi_g\\), \\(\\hat\\mu_g\\) und \\(\\hat\\sigma_g\\) legen jedoch ein einfaches iteratives ML-Schätzverfahren nahe: Nämlich einer alternierenden Schätzung von \\(p_{ig}\\) und \\(\\hat\\pi_g\\), \\(\\hat\\mu_g\\) und \\(\\hat\\sigma_g\\). Der Der EM Algorithmus: Setze Startwerte \\(\\boldsymbol{\\pi}^{(0)}\\), \\(\\boldsymbol{\\mu}^{(0)}\\) und \\(\\boldsymbol{\\sigma}^{(0)}\\) Für \\(r=1,2,\\dots\\) (Expectation) Berechne: \\[p_{ig}^{(r)}=\\frac{\\pi_g^{(r-1)}\\phi_{\\mu^{(r-1)}_g\\sigma_g^{(r-1)}}(x_i)}{f_G(x_i|\\boldsymbol{\\pi}^{(r-1)},\\boldsymbol{\\mu}^{(r-1)},\\boldsymbol{\\sigma}^{(r-1)})}\\] (Maximization) Berechne: \\(\\hat\\pi_g^{(r)}=\\frac{1}{n}\\sum_{i=1}^np_{ig}^{(r)},\\quad\\quad\\hat\\mu_g^{(r)}=\\sum_{i=1}^n\\frac{p_{ig}^{(r)}}{\\left(\\sum_{j=1}^np_{jg}^{(r)}\\right)}x_i\\) \\(\\hat\\sigma_g^{(r)}=\\sqrt{\\sum_{i=1}^n\\frac{p_{ig}^{(r)}}{\\left(\\sum_{j=1}^np_{jg}^{(r)}\\right)}\\left(x_i-\\hat\\mu_g^{(r)}\\right)^2}\\) Prüfe Konvergenz Der obige pseudo-Code wird im Folgenden Code-Chunck umgesetzt: library(&quot;MASS&quot;) library(&quot;mclust&quot;) ## Daten: x &lt;- cbind(Penguine_Flosse) # Daten [n x d]-Dimensional. d &lt;- ncol(x) # Dimension (d=1: univariat) n &lt;- nrow(x) # Stichprobenumfang G &lt;- 2 # Anzahl Gruppen ## Weitere Deklarationen: llk &lt;- matrix(NA, n, G) p &lt;- matrix(NA, n, G) loglikOld &lt;- 1e07 tol &lt;- 1e-05 it &lt;- 0 check &lt;- TRUE ## EM Algorithmus ## 1. Startwerte für pi, mu und sigma: pi &lt;- rep(1/G, G) # Naive pi sigma &lt;- array(diag(d), c(d,d,G)) # Varianz = 1 mu &lt;- t(MASS::mvrnorm(G, colMeans(x), sigma[,,1]*4) ) while(check){ ## 2.a Expectation-Schritt for(g in 1:G){ p[,g] &lt;- pi[g] * mclust:::dmvnorm(x, mu[,g], sigma[,,g]) } p &lt;- sweep(p, 1, STATS = rowSums(p), FUN = &quot;/&quot;) ## 2.b Maximization-Schritt par &lt;- mclust::covw(x, p, normalize = FALSE) mu &lt;- par$mean sigma &lt;- par$S pi &lt;- colMeans(p) ## 3. Prüfung der Konvergenz for(g in 1:G) { llk[,g] &lt;- pi[g] * mclust:::dmvnorm(x, mu[,g], sigma[,,g]) } loglik &lt;- sum(log(rowSums(llk))) # aktueller Log-Likelihood Wert ## diff &lt;- abs(loglik - loglikOld)/abs(loglik) loglikOld &lt;- loglik it &lt;- it + 1 ## Anderung der Log-Likelihood noch groß genug? check &lt;- diff &gt; tol } ## Schätz-Resultate: results &lt;- matrix(c(pi, mu, sqrt(sigma)), nrow = 3, ncol = 2, byrow = TRUE, dimnames = list( c(&quot;Gewichte&quot;, &quot;Mittelwerte&quot;, &quot;Standardabweichungen&quot;), c(&quot;Gruppe 1&quot;, &quot;Gruppe 2&quot;))) ## results %&gt;% round(., 2) #&gt; Gruppe 1 Gruppe 2 #&gt; Gewichte 0.69 0.31 #&gt; Mittelwerte 216.20 194.27 #&gt; Standardabweichungen 7.31 6.27 Das Schätzergebnis erlaubt es uns, Abbildung 1.1 zu replizieren: ## Auswerten der Gaußsche Mischungs-Dichtefunktion np &lt;- 100 # Anzahl der Auswertungspunkte xxd &lt;- seq(min(Penguine_Flosse)-3, max(Penguine_Flosse)+5, length.out = np) ## Mischungs-Dichte yyd &lt;- dnorm(xxd, mu[1,1], sqrt(sigma)[,,1])*pi[1] + dnorm(xxd, mu[1,2], sqrt(sigma)[,,2])*pi[2] ## Einzel-Dichten yyd1 &lt;- dnorm(xxd, mu[1,1], sqrt(sigma)[,,1])*pi[1] yyd2 &lt;- dnorm(xxd, mu[1,2], sqrt(sigma)[,,2])*pi[2] ## Plot hist(x = Penguine_Flosse, xlab=&quot;Flosse (mm)&quot;, main=&quot;Pinguine\\n(Zwei Gruppen)&quot;, col=gray(.65,.5), border=gray(.35,.5), freq = FALSE, ylim=c(0, 0.04)) lines(x = xxd, y=yyd, lwd=2, col=gray(.35,.75)) lines(x = xxd, y=yyd1, lwd=2, col=gray(.35,.75), lty=2) lines(x = xxd, y=yyd2, lwd=2, col=gray(.35,.75), lty=2) abline(v=203.1, lty=3) stripchart(Penguine_Flosse[class==1], method = &quot;jitter&quot;, jitter = .0005, at = .001, pch = 21, col=alpha(col_v[1],.5), bg=alpha(col_v[1],.5), cex=1.3, add = TRUE) stripchart(Penguine_Flosse[class==2], method = &quot;jitter&quot;, jitter = .0005, at = .001, pch = 21, col=alpha(col_v[2],.5), bg=alpha(col_v[2],.5), cex=1.3, add = TRUE) "],
["1-3-der-alternative-wahre-blick-auf-den-em-algorithmus.html", "1.3 Der alternative (wahre) Blick auf den EM-Algorithmus", " 1.3 Der alternative (wahre) Blick auf den EM-Algorithmus Der EM Algorithmus ermöglicht es Maximum Likelihood Probleme zu vereinfachen, indem man die Daten durch nicht beobachtete („latente“) Variablen vervollständigt. Zur Erinnerung: Wie haben es ja nicht geschafft, die Log-Likelihood Funktion \\[ \\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x}) =\\sum_{i=1}^n\\ln\\left(\\sum_{g=1}^G\\pi_g\\phi_{\\mu_g\\sigma_g}(x_i)\\right) \\] direkt zu maximieren. Die \\(\\log(\\sum_{g=1}^G[\\dots])\\)-Konstruktion macht einem das Leben schwer. In unseren Pinguin-Daten gibt zwei Gruppen (\\(g\\in\\{1,2\\}\\)). Ist gäbe im Prinzip also latente (unbeobachtete) Zuordnungsdaten \\(z_{ig}\\) mit \\[ z_{ig}= \\left\\{\\begin{array}{ll} 1&amp;\\text{falls Pinguin }i\\text{ zu Gruppe }g\\text{ gehört.}\\\\ 0&amp;\\text{sonst.}\\\\ \\end{array}\\right., \\] wobei \\(\\sum_{g=1}^Gz_{ig}=1\\) für alle \\(i=1,\\dots,n\\). Beachte: Für jeden Datenpunkt \\(i\\) (jeder Pinguin \\(i\\)) gibt es nur eine Gruppe (daher \\(\\sum_{g=1}^Gz_{ig}=1\\)). Dies ist eine wichtige Restriktion von GMMs, welche bei den Pinguin-Daten unproblematisch ist, in anderen Anwendungen aber evtl. problematisch sein kann. Die Zuordnungsdaten \\(\\mathbf{z}=(z_{11},\\dots,z_{nG})\\) sind leider unbekannt (latent). Wir wissen aber trotzdem etwas über diese Zuordnungen. Laut unserem Modell \\[ f_G(x|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})=\\sum_{g=1}^G\\pi_gf(x|\\mu_g\\sigma_g), \\] ist die Zuordnung \\(z_{ig}\\) nämlich eine Realisation einer Bernoulli Zufallsvariable \\[ Z_{ig}\\sim\\mathcal{B}(\\pi_g) \\] Also \\[\\begin{align*} P(Z_{ig}=1)&amp;=\\;\\;\\;\\pi_g\\;\\;=P(\\text{Pinguin $i$ gehört zu Gruppe }g)\\\\ P(Z_{ig}=0)&amp;=1-\\pi_g=P(\\text{Pinguin $i$ gehört nicht zu Gruppe }g) \\end{align*}\\] Man bezeichnet \\(\\pi_1,\\dots,\\pi_G\\) als die „a-priori-Wahrscheinlichkeiten“. Wenn wir nichts über die Flossenlänge von Pinguin \\(i\\) wissen, dann bleiben uns nur die a-priori-Wahrscheinlichkeiten: Mit Wahrscheinlichkeit \\(\\pi_g\\) gehört Pinguin \\(i\\) zu Gruppe \\(g\\). Falls wir die Flossenlänge von Pinguin \\(i\\) erfahren, können wir die a-priori-Wahrscheinlichkeiten mit Hilfe des Satzes von Bayes aktualisieren. Dies führt dann zur a-posteriori-Wahrscheinlichkeit: A-posteriori-Wahrscheinlichkeit \\(\\;p_{ig}\\): (Satz von Bayes) \\[\\begin{align*} p_{ig} &amp;=\\frac{\\pi_g\\phi_{\\mu_g\\sigma_g}(x_i)}{f_G(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}\\\\[2ex] &amp;=\\frac{\\overbrace{P(Z_{ig}=1)}^{\\text{„A-priori-Wahrs.“}}\\phi_{\\mu_g\\sigma_g}(x_i)}{f_G(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}=\\overbrace{P(Z_{ig}=1|X_i=x_i)}^{\\text{„A-posteriori-Wahrs.“}}=p_{ig}\\\\ \\end{align*}\\] Beachte: \\(p_{ig}\\) ist ein (bedingter) Mittelwert (Expectation) \\[\\begin{align*} p_{ig}&amp;=\\underbrace{1\\cdot P(Z_{ig}=1|X_i=x_i)+0\\cdot P(Z_{ig}=0|X_i=x_i)}_{=E(Z_{ig}=1|X_i=x_i)}\\\\ \\end{align*}\\] Der EM Algorithmus Setze Startwerte \\(\\boldsymbol{\\pi}^{(0)}\\), \\(\\boldsymbol{\\mu}^{(0)}\\) und \\(\\boldsymbol{\\sigma}^{(0)}\\) Für \\(r=1,2,\\dots\\) (Expectation) Berechne: \\(p_{ig}^{(r)}=E\\left(Z_{ig}^{(r-1)}\\left|X_i^{(r-1)}=x_i\\right.\\right)\\) \\[\\begin{align*} \\text{wobei }\\;Z_{ig}^{(r-1)}&amp;\\sim\\mathcal{B}\\left(\\pi_g^{(r-1)}\\right)\\\\ \\text{und }\\;X_i^{(r-1)}&amp;\\sim \\mathcal{N}_{\\mathcal{mix}}(G,\\boldsymbol{\\pi}^{(r-1)},\\boldsymbol{\\mu}^{(r-1)},\\boldsymbol{\\sigma}^{(r-1)}) \\end{align*}\\] (Maximization) Berechne: \\(\\hat\\pi_g^{(r)}\\), \\(\\hat\\mu_g^{(r)}\\), \\(\\hat\\sigma_g^{(r)}\\) Prüfe Konvergenz –&gt; "],
["literatur.html", "Literatur", " Literatur "]
]
