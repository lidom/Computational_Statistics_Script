[["index.html", "Computational Statistics Informationen", " Computational Statistics Prof. Dr. Dominik Liebl Informationen Dies ist das Skript zur Vorlesung Computational Statistik Vorlesungszeiten Wochentag Uhrzeit Hörsaal Dienstag 9:15-10:45 Online-Vorlesung Freitag 8:30-10:00 Online-Vorlesung RCodes Die RCodes zu den einzelnen Kapiteln können hier heruntergeladen werden: RCodes Leseecke Folgende frei zugängliche Lehrbücher enthalten Teile dieses Kurses. In den jeweiligen Kapiteln, werde ich auf die einzelnen Bücher verweisen. Pattern Recognition and Machine Learning (by Christopher Bishop) An Introduction to Statistical Learning, with Applications in R (by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani). Statistical Learning with Sparsity: the Lasso and Generalizations (by Trevor Hastie, Robert Tibshirani and Martin Wainwright). Elements of Statistical Learning: Data mining, Inference and Prediction (by Trevor Hastie, Robert Tibshirani and Jerome Friedman). Computer Age Statistical Inference: Algorithms, Evidence and Data Science (by Bradley Efron and Trevor Hastie) Florence Nightingale Das Logo zu diesem Skript stammt von einer Briefmarke zur Erinnerung an die Krankenschwester und inspirierende Statistikerin, Florence Nightingale. Nightingale war die Begründerin der modernen westlichen Krankenpflege und Pionierin der visuellen Datenanalyse. Sie nutzte statistische Analysen, um Missstände in Kliniken zu erkennen und diese dann auch nachweislich abzustellen. Sie ist die erste Frau, die in die britische Royal Statistical Society aufgenommen wurde; später erhielt sie auch die Ehrenmitgliedschaft der American Statistical Association. Dieses Skript ist lizenziert unter der Creative Commons Lizenz Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["1-der-expectation-maximization-em-algorithmus.html", "1 Der Expectation Maximization (EM) Algorithmus", " 1 Der Expectation Maximization (EM) Algorithmus span { display: inline-block; } Der EM Algorithmus wird häufig verwendet, um komplizierte Maximum Likelihood Schätzprobleme zu vereinfachen bzw. überhaupt erst möglich zu machen. In diesem Kapitel stellen wir den EM Algorithmus zur Schätzung von Gaußschen Mischverteilungen vor, da der EM Algorithmus hier wohl seine bekannteste Anwendung hat. Bereits die originale Arbeit zum EM Algorithmus (Dempster, Laird, und Rubin 1977) beschäftigt sich mit der Schätzung von Gaußschen Mischverteilungen. Mögliche Anwendungen von Gaußschen Mischverteilungen: Generell: Auffinden von Gruppierungen (zwei oder mehr) in den Daten (Clusteranalyse). Zum Beispiel: Automatisierte Videobearbeitungen (z.B. Bildeinteilungen in Vorder- und Hintergrund) Automatisierte Erkennung von Laufstilen etc. Lernziele für dieses Kapitel Sie können … ein Anwendungsfeld des EM Algorithmuses benennen. die Probleme der klassischen Maximum Likelihood Methode zur Schätzung von Gaußschen Mischverteilungen benennen und erläutern. die Grundidee des EM Algorithmuses erläutern. den EM Algorithmus zur Schätzung von Gaußschen Mischverteilungen anwenden. die Grundidee der Vervollständigung der Daten durch latente Variablen erläutern. Begleitlektüre(n) Zur Vorbereitung der Klausur ist es grundsätzlich aussreichend dieses Kapitel durchzuarbeiten - aber Lesen hat ja noch nie geschadet. Dieses Kapitel basiert hauptsächlich auf: Kapitel 9 in Pattern Recognition and Machine Learning (Bishop 2006). Die pdf-Version des Buches ist frei erhältlichen: pdf-Version Weiteren guten Lesestoff zum EM Algoithmus gibt es z.B. hier: Kapitel 8.5 in Elements of Statistical Learning: Data Mining, Inference and Prediction (Hastie, Tibshirani, und Friedman 2009). Die pdf-Version des Buches ist frei erhältlichen: pdf-Version R-Pakete für diese Kapitel Folgende R-Pakete werden für dieses Kapitel benötigt: pkgs &lt;- c(&quot;tidyverse&quot;, # Die tidyverse-Pakete &quot;palmerpenguins&quot;, # Pinguin-Daten &quot;scales&quot;, # Transparente Farben: alpha() &quot;RColorBrewer&quot;, # Hübsche Farben &quot;mclust&quot;, # Schätzung/Verwendung # Gaußschen Mischverteilungen &quot;MASS&quot;) # Erzeugung von Zufallszahlen aus # einer multiv. Normalverteilung install.packages(pkgs) Literatur "],["1.1-motivation-clusteranalyse-mit-hilfe-gaußscher-mischverteilungen.html", "1.1 Motivation: Clusteranalyse mit Hilfe Gaußscher Mischverteilungen", " 1.1 Motivation: Clusteranalyse mit Hilfe Gaußscher Mischverteilungen Als Datenbeispiel verwendent wir die palmerpenguins Daten (Horst, Hill, und Gorman 2020). Diese Daten stammen aus Vermessungen von Pinguinpopulationen auf dem Palmer-Archipel (Antarktische Halbinsel). Pinguine sind oft schwer von einander zu unterscheiden (Abbildung 1.1). Wir werden versuchen, mit Hilfe einer Gaußschen Mischverteilung Gruppierungen in den Pinguindaten (Flossenlänge) zu finden. Um solche Mischverteilungen schätzen zu können, führen wir den EM Algorithmus ein. Abbildung 1.1: Frecher Pinguin bei der Tat. Der folgende Code-Chunck bereitet die Daten auf. Achtung: Wir haben zwar die Information zu den verschiedenen Pinguin-Arten (Penguine_Art) tun aber im Folgenden so, also ob wir diese Information nicht kennen würden. Wir wollen alleine auf Basis der Flossenlängen (Penguine_Flosse) die Gruppenzugehörigkeiten per Clusteranalyse bestimmen. (Im Nachhinein können wir dann mit Hilfe der Daten in Penguine_Art prüfen, wie gut unsere Clusteranalyse ist.) library(&quot;palmerpenguins&quot;) # Pinguin-Daten library(&quot;RColorBrewer&quot;) # Hübsche Farben library(&quot;scales&quot;) # Für transparente Farben: alpha() col_v &lt;- RColorBrewer::brewer.pal(n = 3, name = &quot;Set2&quot;) ## Vorbereitung der Daten: Pinguine &lt;- palmerpenguins::penguins %&gt;% # Pinguin-Daten tidyr::as_tibble() %&gt;% # Datenformat: &#39;tibble&#39;-dataframe dplyr::filter(species!=&quot;Adelie&quot;) %&gt;% # Pinguin-Art &#39;Adelie&#39; löschen droplevels() %&gt;% # Lösche das nicht mehr benötigte Adelie-Level tidyr::drop_na() %&gt;% # NAs löschen dplyr::mutate(Art = species, # Variablen umbenennen Flosse = flipper_length_mm) %&gt;% dplyr::select(Art, Flosse) # Variablen auswählen ## n &lt;- nrow(Pinguine) # Stichprobenumfang ## Variable &#39;Penguine_Art&#39; aus Pinguine-Daten &quot;herausziehen&quot; Penguine_Art &lt;- dplyr::pull(Pinguine, Art) ## Variable &#39;Penguine_Flosse&#39; aus Pinguine-Daten &quot;herausziehen&quot; Penguine_Flosse &lt;- dplyr::pull(Pinguine, Flosse) ## Plot ## Histogramm: hist(x = Penguine_Flosse, freq = FALSE, xlab=&quot;Flossenlänge (mm)&quot;, main=&quot;Pinguine\\n(Zwei Gruppen)&quot;, col=gray(.65,.5), border=gray(.35,.5), ylim=c(0.0003, 0.039)) ## Stipchart hinzufügen: stripchart(x = Penguine_Flosse, method = &quot;jitter&quot;, jitter = .0005, at = .001, pch = 21, col=alpha(col_v[3],.5), bg=alpha(col_v[3],.5), cex=1.3, add = TRUE) Das Clusterverfahren basierend auf Gaußschen Mischverteilungen: Gaußsche Mischverteilung (per EM Algorithmus) schätzen Die Datenpunkte \\(x_i\\) derjenigen Gruppe zuordnen, welche die „a-posteriori-Wahrscheinlichkeit“ maximiert (siehe Abbildung 1.2) Abbildung 1.2: Clusteranalyse basierend auf einer Mischverteilung mit zwei gewichteten Normalverteilungen. Abbildung 1.2 zeigt das Resultat einer Clusteranalyse basierend auf einer Mischverteilung zweier gewichteter Normalverteilungen. Cluster-Ergebnis: 95% der Pinguine konnten richtig zugeordnet werden - lediglich auf Basis ihrer Flossenlängen. Mit Hilfe der folgenden R-Codes kann die obige Clusteranalyse und die Ergebnisgrafik repliziert werden: ## mclust R-Paket: ## Clusteranalyse mit Hilfe von Gaußschen Mischmodellen suppressMessages(library(&quot;mclust&quot;)) ## Anzahl der Gruppen G &lt;- 2 ## Schätzung des Gaußschen Mischmodells (per EM Algorithmus) ## und Clusteranalyse mclust_obj &lt;- mclust::Mclust(data = Penguine_Flosse, G=G, modelNames = &quot;V&quot;, verbose = FALSE) # summary(mclust_obj) # str(mclust_obj) ## Geschätzte Gruppen-Zuordnungen (Cluster-Resultat) class &lt;- mclust_obj$classification ## Anteil der korrekten Zuordnungen: # cbind(class, Penguine_Art) round(sum(class == as.numeric(Penguine_Art))/n, 2) ## Geschätzte Mittelwerte mean_m &lt;- t(mclust_obj$parameters$mean) ## Geschätzte Varianzen (und evtl. Kovarianzen) cov_l &lt;- list(&quot;Cov1&quot; = mclust_obj$parameters$variance$sigmasq[1], &quot;Cov2&quot; = mclust_obj$parameters$variance$sigmasq[2]) ## Geschätzte Gewichte (a-priori-Wahrscheinlichkeiten) prop_v &lt;- mclust_obj$parameters$pro ## Auswerten der Gaußsche Mischung-Dichtefunktion np &lt;- 100 # Anzahl der Auswertungspunkte xxd &lt;- seq(min(Penguine_Flosse)-3, max(Penguine_Flosse)+5, length.out = np) ## Mischungs-Dichte yyd &lt;- dnorm(xxd, mean_m[1], sqrt(cov_l[[1]]))*prop_v[1] + dnorm(xxd, mean_m[2], sqrt(cov_l[[2]]))*prop_v[2] ## Einzel-Dichten yyd1 &lt;- dnorm(xxd, mean_m[1], sqrt(cov_l[[1]]))*prop_v[1] yyd2 &lt;- dnorm(xxd, mean_m[2], sqrt(cov_l[[2]]))*prop_v[2] ## Plot hist(x = Penguine_Flosse, xlab=&quot;Flossenlänge (mm)&quot;, main=&quot;Pinguine\\n(Zwei Gruppen)&quot;, col=gray(.65,.5), border=gray(.35,.5), freq = FALSE, ylim=c(0, 0.04)) lines(x = xxd, y=yyd, lwd=2, col=gray(.35,.75)) lines(x = xxd, y=yyd1, lwd=2, col=gray(.35,.75), lty=2) lines(x = xxd, y=yyd2, lwd=2, col=gray(.35,.75), lty=2) stripchart(Penguine_Flosse[class==1], method = &quot;jitter&quot;, jitter = .0005, at = .001, pch = 21, col=alpha(col_v[1],.5), bg=alpha(col_v[1],.5), cex=1.3, add = TRUE) stripchart(Penguine_Flosse[class==2], method = &quot;jitter&quot;, jitter = .0005, at = .001, pch = 21, col=alpha(col_v[2],.5), bg=alpha(col_v[2],.5), cex=1.3, add = TRUE) Literatur "],["1.2-der-em-algorithmus-zur-ml-schätzung-gaußscher-mischverteilungen.html", "1.2 Der EM Algorithmus zur ML-Schätzung Gaußscher Mischverteilungen", " 1.2 Der EM Algorithmus zur ML-Schätzung Gaußscher Mischverteilungen 1.2.1 Gaußsche Mischmodelle (GMM) Eine Zufallsvariable \\(X\\), die einer Gauschen Mischverteilung folgt, bezeichnen wir als \\[ X\\sim\\mathcal{N}_{\\mathcal{mix}}(G,\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}) \\] Die dazugehörige Dichtefunktion einer Gaußschen Mischverteilung ist folgendermaßen definiert: \\[\\begin{equation} f_G(x|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})=\\sum_{g=1}^G\\pi_gf(x|\\mu_g,\\sigma_g) \\tag{1.1} \\end{equation}\\] Gewichte: \\(\\boldsymbol{\\pi}=(\\pi_1,\\dots,\\pi_G)\\) mit \\(\\pi_g&gt;0\\) und \\(\\sum_{g=1}^G\\pi_g=1\\) Mittelwerte: \\(\\boldsymbol{\\mu}=(\\mu_1,\\dots,\\mu_G)\\) mit \\(\\mu_g\\in\\mathbb{R}\\) Standardabweichungen: \\(\\boldsymbol{\\sigma}=(\\sigma_1,\\dots,\\sigma_G)\\) mit \\(\\sigma_g&gt;0\\) Normalverteilung der Gruppe \\(g=1,\\dots,G\\): \\[ f(x|\\mu_g,\\sigma_g)=\\frac{1}{\\sqrt{2\\pi}\\sigma_g}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu_g}{\\sigma_g}\\right)^2\\right) \\] Unbekannte Parameter: \\(\\boldsymbol{\\pi}\\), \\(\\boldsymbol{\\mu}\\) und \\(\\boldsymbol{\\sigma}\\) 1.2.2 Maximum Likelihood (ML) Schätzung Man kann versuchen die unbekannten Parameter \\(\\boldsymbol{\\pi}=(\\pi_1,\\dots,\\pi_G)\\), \\(\\boldsymbol{\\mu}=(\\mu_1,\\dots,\\mu_G)\\) und \\(\\boldsymbol{\\sigma}=(\\sigma_1,\\dots,\\sigma_G)\\) eines Gaußschen Mischmodells klassisch mit Hilfe der Maximum Likelihood Methode zu schätzen. Ich sag’s gleich: Der Versuch wird scheitern. Wiederholung der Grundidee der ML-Schätzung: Annahme: Die Daten \\(\\mathbf{x}=(x_1,\\dots,x_n)\\) sind eine Realisation einer einfachen (also i.i.d.) Zufallsstichprobe \\((X_1,\\dots,X_n)\\) mit \\[ X_i\\sim\\mathcal{N}_{\\mathcal{mix}}(G,\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}) \\] für alle \\(i=1,\\dots,n\\). Die Daten \\(\\mathbf{x}=(x_1,\\dots,x_n)\\) „kennen“ also die unbekannten Parameter \\(\\boldsymbol{\\pi},\\) \\(\\boldsymbol{\\mu}\\) und \\(\\boldsymbol{\\sigma}\\) und wir müssen ihnen diese Informationen „nur noch“ entlocken. Schätz-Idee: Wähle \\(\\boldsymbol{\\pi}\\), \\(\\boldsymbol{\\mu}\\) und \\(\\boldsymbol{\\sigma}\\) so, dass \\(f_G(\\cdot|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\) „optimal“ zu den beobachteten Daten \\(\\mathbf{x}\\) passt. Umsetzung der Schätz-Idee: Maximiere (bzgl. \\(\\boldsymbol{\\pi}\\), \\(\\boldsymbol{\\mu}\\) und \\(\\boldsymbol{\\sigma}\\)) die Likelihood Funktion \\[\\mathcal{L}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x})=\\prod_{i=1}^nf_G(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\] Bzw. maximiere die Log-Likelihood Funktion (einfachere Maximierung) \\[\\begin{align*} \\ln\\left(\\mathcal{L}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x})\\right)= \\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x}) =&amp;\\sum_{i=1}^n\\ln\\left(f_G(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\right)\\\\ =&amp;\\sum_{i=1}^n\\ln\\left(\\sum_{g=1}^G\\pi_gf(x_i|\\mu_g,\\sigma_g)\\right) \\end{align*}\\] Beachte: Die Maximierung muss die Parameterrestriktionen in (1.1) berücksichtigen (\\(\\sigma_g&gt;0\\) und \\(\\pi_g&gt;0\\) für alle \\(g=1,\\dots,G\\) und \\(\\sum_{g=1}^G\\pi_g=1\\)). Die maximierenden Parameterwerte \\(\\hat{\\boldsymbol{\\pi}}\\), \\(\\hat{\\boldsymbol{\\mu}}\\) und \\(\\hat{\\boldsymbol{\\sigma}}\\) sind die ML-Schätzer. Das kann man so ausdrücken: \\[ (\\hat{\\boldsymbol{\\pi}},\\hat{\\boldsymbol{\\mu}},\\hat{\\boldsymbol{\\sigma}})=\\arg\\max_{\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}}\\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x}) \\] 😒 Probleme mit Singularitäten bei Numerische Lösungen: Versucht man obiges Maximierungsproblem numerisch mit Hilfe des Computers zu lösen, wird man schnell merken, dass die Ergebnisse höchst instabil, unplausibel und wenig vertrauenswürdig sind. Der Grund für diese instabilen Schätzungen sind Probleme mit Singularitäten: Für echte GMMs (\\(G&gt;1\\)) treten während einer numerischen Maximierung sehr leicht Probleme mit Singularitäten auf. Dies geschieht immer dann, wenn eine der Normalverteilungskomponenten versucht den ganzen Datensatz \\(\\mathbf{x}\\) zu beschreiben und die andere(n) Normalverteilungskomponente(n) versuchen lediglich einzelne Datenpunkte zu beschreiben. Eine Gaußsche Dichtefunktion, die sich um einen einzigen Datenpunkt \\(x_i\\) konzentriert (d.h. \\(\\mu_g=x_i\\) und \\(\\sigma_g\\to 0\\)) wird dabei sehr große Werte annehmen (d.h. \\(f(x_i|\\mu_g=x_i,\\sigma_g)\\to\\infty\\) für \\(\\sigma_g\\to 0\\)) und so die Log-Likelihood auf unerwünschte Weise maximieren (siehe Abbildung 1.3). Solch unerwünschte, triviale Maximierungslösungen führen i.d.R. zu unplausiblen Schätzergebnissen. Abbildung 1.3: Normalverteilung mit \\(\\mu_g=x_i\\) für \\(\\sigma_g\\to 0\\). 😒 Analytische Lösung: Es ist zwar etwas mühsam, aber man kann versuchen die Log-Likelihood analytisch zu maximieren. Tut man sich das an, kommt man zu folgenden Ausdrücken: \\[\\begin{align*} \\hat\\pi_g&amp;=\\frac{1}{n}\\sum_{i=1}^np_{ig}\\\\ \\hat\\mu_g&amp;=\\sum_{i=1}^n\\frac{p_{ig}}{\\left(\\sum_{j=1}^np_{jg}\\right)}x_i\\\\ \\hat\\sigma_g&amp;=\\sqrt{\\sum_{i=1}^n\\frac{p_{ig}}{\\left(\\sum_{j=1}^np_{jg}\\right)}\\left(x_i-\\hat\\mu_g\\right)^2} \\end{align*}\\] für \\(g=1,\\dots,G\\). Die Herleitung der Ausdrücke für \\(\\hat{\\mu}_g\\), \\(\\hat{\\sigma}_g\\) und \\(\\hat{\\pi}_g\\) ist wirklich etwas lästig (mehrfache Anwendungen der Kettenregel, Produktregel, etc., sowie Anwendung des Lagrange-Multiplikator Verfahrens zur Optimierung unter Nebenbedingungen) aber machbar. In einer der Übungsaufgaben dürfen Sie den Ausdruck für \\(\\hat\\mu_g\\) herleiten. 🙈 Aber: Diese Ausdrücke für \\(\\hat\\pi_g\\), \\(\\hat\\mu_g\\) und \\(\\hat\\sigma_g\\) hängen von den unbekannten Parametern \\(\\boldsymbol{\\pi}=(\\pi_1,\\dots,\\pi_G)\\), \\(\\boldsymbol{\\mu}=(\\mu_1,\\dots,\\mu_G)\\) und \\(\\boldsymbol{\\sigma}=(\\sigma_1,\\dots,\\sigma_G)\\), denn: \\[ p_{ig}=\\frac{\\pi_gf(x_i|\\mu_g,\\sigma_g)}{f_G(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})} \\] für \\(i=1,\\dots,n\\) und \\(g=1,\\dots,G\\). Die Ausdrücke für \\(\\hat\\pi_g\\), \\(\\hat\\mu_g\\), und \\(\\hat\\sigma_g\\) erlauben also keine keine direkte Schätzung der unbekannten Parameter \\(\\pi_g\\), \\(\\mu_g\\) und \\(\\sigma_g\\). 🥳 Lösung: Der EM Algorithmus 1.2.3 Der EM Algorithmus für GMMs Die Ausdrücke für \\(\\hat\\pi_g\\), \\(\\hat\\mu_g\\) und \\(\\hat\\sigma_g\\) legen jedoch ein einfaches, iteratives ML-Schätzverfahren nahe: Nämlich einer alternierenden Schätzung von \\(p_{ig}\\) und \\((\\hat\\pi_g, \\hat\\mu_g,\\hat\\sigma_g)\\). Der Der EM Algorithmus: Setze Startwerte \\(\\boldsymbol{\\pi}^{(0)}\\), \\(\\boldsymbol{\\mu}^{(0)}\\) und \\(\\boldsymbol{\\sigma}^{(0)}\\) Für \\(r=1,2,\\dots\\) (Expectation) Berechne: \\[p_{ig}^{(r)}=\\frac{\\pi_g^{(r-1)}f(x_i|\\mu^{(r-1)}_g,\\sigma_g^{(r-1)})}{f_G(x_i|\\boldsymbol{\\pi}^{(r-1)},\\boldsymbol{\\mu}^{(r-1)},\\boldsymbol{\\sigma}^{(r-1)})}\\] (Maximization) Berechne: \\(\\hat\\pi_g^{(r)}=\\frac{1}{n}\\sum_{i=1}^np_{ig}^{(r)},\\quad\\quad\\hat\\mu_g^{(r)}=\\sum_{i=1}^n\\frac{p_{ig}^{(r)}}{\\left(\\sum_{j=1}^np_{jg}^{(r)}\\right)}x_i\\) \\(\\hat\\sigma_g^{(r)}=\\sqrt{\\sum_{i=1}^n\\frac{p_{ig}^{(r)}}{\\left(\\sum_{j=1}^np_{jg}^{(r)}\\right)}\\left(x_i-\\hat\\mu_g^{(r)}\\right)^2}\\) Prüfe Konvergenz: Stoppe, falls sich der Wert der maximierten Log-Likelihood Funktion, \\(\\ell(\\boldsymbol{\\pi}^{(r)},\\boldsymbol{\\mu}^{(r)},\\boldsymbol{\\sigma}^{(r)}|\\mathbf{z})\\), nicht mehr ändert. Der obige pseudo-Code wird im Folgenden Code-Chunck umgesetzt: library(&quot;MASS&quot;) library(&quot;mclust&quot;) ## Daten: x &lt;- cbind(Penguine_Flosse) # Daten [n x d]-Dimensional. d &lt;- ncol(x) # Dimension (d=1: univariat) n &lt;- nrow(x) # Stichprobenumfang G &lt;- 2 # Anzahl Gruppen ## Weitere Deklarationen: llk &lt;- matrix(NA, n, G) p &lt;- matrix(NA, n, G) loglikOld &lt;- 1e07 tol &lt;- 1e-05 it &lt;- 0 check &lt;- TRUE ## EM Algorithmus ## 1. Startwerte für pi, mu und sigma: pi &lt;- rep(1/G, G) # Naive pi sigma &lt;- array(diag(d), c(d,d,G)) # Varianz = 1 mu &lt;- t(MASS::mvrnorm(G, colMeans(x), sigma[,,1]*4) ) while(check){ ## 2.a Expectation-Schritt for(g in 1:G){ p[,g] &lt;- pi[g] * mclust:::dmvnorm(x, mu[,g], sigma[,,g]) } p &lt;- sweep(p, 1, STATS = rowSums(p), FUN = &quot;/&quot;) ## 2.b Maximization-Schritt par &lt;- mclust::covw(x, p, normalize = FALSE) mu &lt;- par$mean sigma &lt;- par$S pi &lt;- colMeans(p) ## 3. Prüfung der Konvergenz for(g in 1:G) { llk[,g] &lt;- pi[g] * mclust:::dmvnorm(x, mu[,g], sigma[,,g]) } loglik &lt;- sum(log(rowSums(llk))) # aktueller max. Log-Likelihood Wert ## diff &lt;- abs(loglik - loglikOld)/abs(loglik) # Änderungsrate loglikOld &lt;- loglik it &lt;- it + 1 ## Änderungsrate noch groß genug (&gt; tol)? check &lt;- diff &gt; tol } ## Schätz-Resultate: results &lt;- matrix(c(pi, mu, sqrt(sigma)), nrow = 3, ncol = 2, byrow = TRUE, dimnames = list( c(&quot;Gewichte&quot;, &quot;Mittelwerte&quot;, &quot;Standardabweichungen&quot;), c(&quot;Gruppe 1&quot;, &quot;Gruppe 2&quot;))) ## results %&gt;% round(., 2) #&gt; Gruppe 1 Gruppe 2 #&gt; Gewichte 0.31 0.69 #&gt; Mittelwerte 194.27 216.20 #&gt; Standardabweichungen 6.27 7.32 Das Clusterverfahren basierend auf Gaußschen Mischverteilungen: Gaußsche Mischverteilung (per EM Algorithmus) schätzen Die Datenpunkte \\(x_i\\) derjenigen Gruppe \\(g\\) zuordnen, welche die a-posteriori-Wahrscheinlichkeit \\(p_{ig}\\) maximiert. Abbildung 1.4 visualisiert den Fortschritt der iterativen Schätzung mit Hilfe des EM Algorithmuses. Abbildung 1.4: Iterative Schätzung mit Hilfe des EM Algorithmuses. Das finale Schätzergebnis erlaubt es uns, Abbildung 1.2 zu replizieren. "],["1.3-der-alternative-wahre-blick-auf-den-em-algorithmus.html", "1.3 Der alternative (wahre) Blick auf den EM Algorithmus", " 1.3 Der alternative (wahre) Blick auf den EM Algorithmus Der EM Algorithmus ermöglicht es Maximum Likelihood Probleme zu vereinfachen, indem man die Daten durch nicht beobachtete („latente“) Variablen vervollständigt. Dieses Prinzip ist der eigentliche wahre Beitrag des EM Algorithmus. Es ermöglicht die Lösung verschiedener Maximum Likelihood Probleme - wir bleiben aber hier bei der Schätzung von GMMs. Zur Erinnerung: Wir haben es ja nicht geschafft, die Log-Likelihood Funktion \\[ \\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x}) =\\sum_{i=1}^n\\ln\\left(\\sum_{g=1}^G\\pi_gf(x_i|\\mu_g,\\sigma_g)\\right) \\] direkt zu maximieren. Die \\(\\ln(\\sum_{g=1}^G[\\dots])\\)-Konstruktion macht einem hier das Leben schwer. 1.3.1 Vervollständigung der Daten In unseren Pinguin-Daten gibt zwei Gruppen (\\(g\\in\\{1,2\\}\\)). Es gäbe also im Prinzip \\(G=2\\)-dimensionale Zuordnungsvektoren \\((z_{i1},z_{i2})\\) mit \\[ (z_{i1},z_{i2})= \\left\\{\\begin{array}{ll} (1,0)&amp;\\text{falls Pinguin }i\\text{ zu Gruppe }g=1\\text{ gehört.}\\\\ (0,1)&amp;\\text{falls Pinguin }i\\text{ zu Gruppe }g=2\\text{ gehört.}\\\\ \\end{array}\\right. \\] Im Fall von \\(G&gt;2\\) Gruppen: \\[ (z_{i1},\\dots,z_{ig},\\dots,z_{iG})= \\left\\{\\begin{array}{ll} (1,0,\\dots,0)&amp;\\text{falls Datenpunkt }i\\text{ zu Gruppe }g=1\\text{ gehört.}\\\\ (0,1,\\dots,0)&amp;\\text{falls Datenpunkt }i\\text{ zu Gruppe }g=2\\text{ gehört.}\\\\ \\quad\\quad\\vdots&amp;\\\\ (0,0,\\dots,1)&amp;\\text{falls Datenpunkt }i\\text{ zu Gruppe }g=G\\text{ gehört.}\\\\ \\end{array}\\right. \\] Die Zuordnungen \\(z_{ig}\\) können also die Werte \\(z_{ig}\\in\\{0,1\\}\\) annehmen, wobei aber gelten muss, dass \\(\\sum_{g=1}^Gz_{ig}=1\\). Beachte: Für jeden Datenpunkt \\(i\\) (jeder Pinguin \\(i\\)) gibt es nur eine Gruppe (daher \\(\\sum_{g=1}^Gz_{ig}=1\\)). Dies ist eine wichtige Restriktion von GMMs, welche bei den Pinguindaten unproblematisch ist. Bei Anwendungen mit hirarchischen Gruppierungen ist dies aber evtl. problematisch. Die Zuordnungen \\(z_{ig}\\) sind leider unbekannt (latent). Wir wissen aber trotzdem etwas über diese Zuordnungen. Die Gewichte \\(\\pi_1,\\dots,\\pi_G\\) der Gaußschen Mischverteilung \\[ f_G(x|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})=\\sum_{g=1}^G\\pi_gf(x|\\mu_g,\\sigma_g), \\] geben die Anteile der Einzelverteilungen \\(f(\\cdot|\\mu_g,\\sigma_g)\\) an der Gesamtverteilung \\(f_G\\) an. Im Schnitt kommen also \\(\\pi_g\\cdot 100\\%\\) der Datenpunkte von Gruppe \\(g\\), \\(g=1,\\dots,G\\). Somit können wir die (latente) Zuordnung \\(z_{ig}\\) als eine Realisation der Zufallsvariablen \\(Z_{ig}\\) mit Wahrscheinlichkeitsfunktion \\[ P(Z_{ig}=1)=\\pi_g \\] auffassen. Wegen der Bedingung \\(\\sum_{g=1}^Gz_{ig}=1\\), gilt dass \\[ Z_{ig}=1\\quad \\Rightarrow\\quad Z_{i1}=0,\\dots,Z_{ig-1}=0,Z_{ig+1}=0,\\dots,Z_{iG}=0. \\] 1.3.2 A-priori und A-posteriori Wahrscheinlichkeiten: \\(\\pi_g\\) und \\(p_{ig}\\) A-priori-Wahrscheinlichkeit \\(\\pi_g\\): Man bezeichnet die Wahrscheinlichkeiten \\(\\pi_g\\) als die a-priori-Wahrscheinlichkeiten. Wenn wir nichts über die Flossenlänge von Pinguin \\(i\\) wissen, dann bleiben uns nur die a-priori-Wahrscheinlichkeiten: „Mit Wahrscheinlichkeit \\(\\pi_g=P(Z_{ig}=1)\\) gehört Pinguin \\(i\\) zu Gruppe \\(g\\).“ A-posteriori-Wahrscheinlichkeit \\(\\;p_{ig}\\): Falls wir die Flossenlänge \\(x_i\\) von Pinguin \\(i\\) erfahren, können wir die a-priori-Wahrscheinlichkeiten mit Hilfe des Satzes von Bayes aktualisieren. Dies führt dann zur a-posteriori-Wahrscheinlichkeit: „Mit Wahrscheinlichkeit \\(p_{ig}=P(Z_{ig=1}|X_i=x_i)\\) gehört ein Pinguin \\(i\\) mit Flossenlänge \\(x_i\\) zu Gruppe \\(g\\). Satz von Bayes: \\[\\begin{align*} p_{ig} &amp;=\\frac{\\pi_gf(x_i|\\mu_g,\\sigma_g)}{f_G(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}\\\\[2ex] &amp;=\\frac{\\overbrace{P(Z_{ig}=1)}^{\\text{„A-priori-Wahrs.“}}f(x_i|\\mu_g,\\sigma_g)}{f_G(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}=\\overbrace{P(Z_{ig}=1|X_i=x_i)}^{\\text{„A-posteriori-Wahrs.“}}=p_{ig}\\\\ \\end{align*}\\] 1.3.3 Der (bedingte) Mittelwert: \\(p_{ig}\\) Beachte: Die a-posteriori-Wahrscheinlichkeiten \\(p_{ig}\\) sind tatsächlich (bedingte) Erwartungswerte: \\[\\begin{align*} p_{ig}&amp;= 1\\cdot P(Z_{ig}=1|X_i=x_i)+0\\cdot P(Z_{ig}=0|X_i=x_i) = E(Z_{ig}|X_i=x_i)\\\\ \\end{align*}\\] Damit ist die Berechnung von \\(p_{ig}\\) im (Expectation)-Schritt des EM Algorithmuses (siehe Kapitel 1.2.3) also tatsächlich eine Erwartungswertberechnung. "],["1.4-das-große-ganze.html", "1.4 Das Große Ganze", " 1.4 Das Große Ganze Hätten wir neben den Datenpunkten \\(\\mathbf{x}=(x_1,\\dots,x_n)\\) auch die Gruppenzuordnungen \\(\\mathbf{z}=(z_{11},\\dots,z_{nG})\\) beobachtet, dann könnten wir folgende Likelihood (\\(\\tilde{\\mathcal{L}}\\)) bzw. Log-Likelihood (\\(\\tilde{\\ell}\\)) Funktion aufstellen: \\[\\begin{align*} \\tilde{\\mathcal{L}}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x},\\mathbf{z}) &amp;=\\prod_{i=1}^n\\prod_{g=1}^G\\left(\\pi_gf(x_i|\\mu_g,\\sigma_g)\\right)^{z_{ig}}\\\\[2ex] \\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x},\\mathbf{z}) &amp;=\\sum_{i=1}^n\\sum_{g=1}^Gz_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(f(x_i|\\mu_g,\\sigma_g)\\right)\\right\\} \\end{align*}\\] Im Gegensatz zur ursprünglichen Log-Likelihood Funktion (\\(\\ell\\)), wäre die neue Log-Likelihood Funktion \\(\\tilde\\ell\\) einfach zu maximieren, da hier keine Summe innerhalb der Logarithmusfunktion steckt, sodass wir direkt den Logarithmus der Normalverteilung berechnen können. Dies vereinfacht das Maximierungsproblem deutlich, da die Normalverteilung zur Exponentialfamilie gehört. Aber: Wir beobachten die Realisationen \\(\\mathbf{z}=(z_{11},\\dots,z_{nG})\\) nicht, sondern kennen lediglich die Verteilung der Zufallsvariablen \\(\\mathbf{Z}=(Z_{11},\\dots,Z_{nG})\\). Dies führt zu einer stochastischen Version der Log-Likelihood Funktion: \\[ \\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x},\\mathbf{Z})=\\sum_{i=1}^n\\sum_{g=1}^GZ_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(f(x_i|\\mu_g,\\sigma_g)\\right)\\right\\} \\] Von dieser können jedoch den bedingten Erwartungswert berechnen: \\[ E(\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x},\\mathbf{z})|X_i=x_i)=\\sum_{i=1}^n\\sum_{g=1}^Gp_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(f(x_i|\\mu_g,\\sigma_g)\\right)\\right\\} \\] 1.4.1 Der EM Algorithmus: Die abstrakte Version Der folgende EM Algorithmus unterscheidet sich wieder lediglich in der Notation von den oben bereits besprochenen Versionen (siehe Kapitel 1.2.3). Die hier gewählte Notation verdeutlicht, dass der Expectation-Schritt die zu maximierende Log-Likelihood Funktion aktualisiert und diese dann im (Maximization)-Schritt maximiert wird. Darüber hinaus ist die gewählte Notation abstrakt genug, um die Grundidee des EM Algorithmuses auf andere Maximum Likelihood Probleme zu übertragen. Im Folgenden wird der Parametervektor \\((\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\) der Einfachheit halber auch mit \\(\\boldsymbol{\\theta}\\) bezeichnet. Setze Startwerte \\(\\boldsymbol{\\theta}^{(0)}=(\\pi^{(0)}, \\mu^{(0)}, \\sigma^{(0)})\\) Für \\(r=1,2,\\dots\\) (Expectation) Berechne: \\[\\begin{align*} \\mathcal{Q}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{(r-1)}) &amp;=E_{\\boldsymbol{\\theta}^{(r-1)}}(\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x},\\mathbf{z})|X_i=x_i)\\\\ &amp;=\\sum_{i=1}^n\\sum_{k=1}^Kp_{ig}^{(r-1)}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(f(x_i|\\mu_g,\\sigma_g)\\right)\\right\\} \\end{align*}\\] (Maximization) Berechne: \\[\\begin{align*} \\boldsymbol{\\theta}^{(r)}=\\arg\\max_{\\boldsymbol{\\theta}}\\mathcal{Q}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{(r-1)}) \\end{align*}\\] Prüfe Konvergenz: Stoppe, falls sich der Wert der maximierten Log-Likelihood Funktion, \\(\\mathcal{Q}(\\boldsymbol{\\theta}^{(r)},\\boldsymbol{\\theta}^{(r-1)})\\), nicht mehr ändert. Ende Dem gemeinen Pinguin ist der EM Algorithmus egal (Abbildung 1.5). Abbildung 1.5: Pinguinforschung am Limit. "],["2-ch:RegML.html", "2 Regressionsmodelle im Kontext des Maschinellen Lernens", " 2 Regressionsmodelle im Kontext des Maschinellen Lernens span { display: inline-block; } Lineare Regressionsmodelle gehören zu den erfolgreichsten statistischen Modellen, da sie vergleichsweise einfach zu interpretieren sind und zugleich äußerst flexibel sind. In diesem Kapitel betrachten wir das multivariate (oder multiple) lineare Regressionsmodell als Prädiktionsmodell im Kontext des maschinellen Lernens. Lernziele für dieses Kapitel Sie können … ein Anwendungsfeld des linearen Regressionsmodell als Prädiktionsmodell benennen. die Probleme der Auswahl eines geeigneten Prädiktionsmodells am Beispiel der Polynomregression benennen und erläutern. die Grundidee der Validierungsdaten-Methode erläutern. die Grundidee der k-fachen Kreuzvalidierung erläutern. Begleitlektüren Zur Vorbereitung der Klausur ist es grundsätzlich ausreichend dieses Kapitel durchzuarbeiten - aber Lesen hat ja noch nie geschadet. Empfehlenswerte weiterführende Literatur: Kapitel 3 in An Introduction to Statistical Learning, with Applications in R (James u. a. 2021). Die pdf-Version des Buches ist hier frei erhältlichen: www.statlearning.com Kapitel 3 in Pattern Recognition and Machine Learning (Bishop 2006). Die pdf-Version des Buches ist frei erhältlichen: pdf-Version Kapitel 6 in Introduction to Econometrics with R (Hanck u. a. 2021). Freies Online-Buch: www.econometrics-with-r.org R-Pakete und Datenbeispiel für dieses Kapitel Abbildung 2.1: Abgase am Morgen (Foto: David Lee). Folgende Pakete werden in diesem Kapitel benötigt. tidyverse: Viele nützliche Pakete zur Datenverarbeitung. GGally: Enthält die Funktion ggpairs() zur Erzeugung von Pairs-Plots ISLR: Enthält die Auto Daten Falls noch nicht geschehen, müssen diese Pakete installiert und geladen werden: ## Installieren install.packages(&quot;tidyverse&quot;) install.packages(&quot;GGally&quot;) install.packages(&quot;ISLR&quot;) ## Laden library(&quot;tidyverse&quot;) # Viele nützliche Pakete zur Datenverarbeitung library(&quot;GGally&quot;) # Pairs-Plot library(&quot;ISLR&quot;) # Enthält die Auto-Daten data(Auto) # Auto-Daten abrufbar machen Als Datenbeispiel für diese Kapitel betrachten wir den Auto Datensatz im R-Paket ISLR. Wir betrachten folgende Auswahl der Variablen im Datensatz Auto: Zielvariable: Verbrauch (km/Liter) Prädiktorvariablen: Gewicht (kg): Schwerere Autos verbrauchen wahrscheinlich mehr. Leistung (PS): Höhere Leistung geht wohl auch mit höherem Verbrauch einher. Hubraum (ccm): Großer Hubraum … höherer Verbrauch? Achtung: Es gibt sicherlich noch weitere relevante Prädiktorvariablen. Obige Auswahl ist jedoch relativ einfach zu erheben und ermöglicht eventuell bereits eine gute Prädiktion des Verbrauches im Rahmen eines Regressionsmodells. Falls dem so ist, könnte unser Prädiktionsmodell dazu dienen, nach Auffälligkeiten bei den herstellerseitigen Verbrauchsangaben zu suchen. Besonders große Abweichung zwischen Modellprädiktion und Herstellerangabe wäre ein mögliches Indiz auf unlautere Zahlenschönung. Aufbereitung der Daten: ## Auswahl und Aufbereitung der Variablen Auto_df &lt;- Auto %&gt;% mutate(Verbrauch = mpg * (1.60934/3.78541), # Verbrauch (km/Liter) Gewicht = weight * 0.45359, # Gewicht (kg) PS = horsepower, # Pferdestärken (PS) Hubraum = displacement * 2.54^3 # Hubraum (ccm) ) %&gt;% dplyr::select(&quot;Verbrauch&quot;, &quot;Gewicht&quot;, &quot;PS&quot;, &quot;Hubraum&quot;) n &lt;- nrow(Auto_df) # Stichprobenumfang Insgesamt enthält der betrachtete Datensatz also fünf Variablen zu \\(n=392\\) verschiedenen Autos. Dies sind die ersten sechs Zeilen des Datensatzes: Verbrauch (km/Liter) Gewicht (kg) Pferdestärken (PS) Hubraum (ccm) 7.65 1589.38 130 5030.83 6.38 1675.11 165 5735.47 7.65 1558.54 150 5211.09 6.80 1557.17 150 4981.67 7.23 1564.43 140 4948.89 6.38 1969.03 198 7030.05 Um sich einen Überblick zu den Beziehungen zwischen den Variablen zu verschaffen, eignet sich ein Pairs-Plot sehr gut (siehe Abbildung 2.2): ggpairs(Auto_df, upper = list(continuous = &quot;density&quot;, combo = &quot;box_no_facet&quot;), lower = list(continuous = &quot;points&quot;, combo = &quot;dot_no_facet&quot;)) Abbildung 2.2: Pairs-Plot zur Veranschaulichung der paarweisen Zusammenhänge zwischen den Variablen. Der Pairs-Plot veranschaulicht alle paarweisen Zusammenhänge zwischen den Variablen im Datensatz Auto_df. Uns interessieren hierbei in erster Linie die Zusammenhänge zwischen der Zielvariable Verbrauch und den Prädiktorvariablen: \\(Y=\\)Verbrauch und … \\(G=\\) Gewicht\\(_i\\): haben einen nicht linearen, negativen Zusammenhang. \\(P=\\) PS: haben einen nicht linearen, negativen Zusammenhang. \\(H=\\) Hubraum: haben einen nicht linearen, negativen Zusammenhang. Literatur "],["2.1-das-allgemeine-regressionsmodell.html", "2.1 Das allgemeine Regressionsmodell", " 2.1 Das allgemeine Regressionsmodell Die einzelnen Prädiktorvariablen werden gerne kompakt zu einer multivariaten Prädiktorvariablen \\(X=(X_1,X_2,\\dots,X_p)\\) zusammengefasst; in unserem Benzinverbrauchsbeispiel also \\(X=(G,P,H,B)\\). So lässt sich das allgemeines Regressionsmodell schreiben als \\[ Y=f(X)+\\varepsilon \\] wobei \\(f\\) den systematischen Zusammenhang zwischen der Zielvariable \\(Y\\) und den Prädiktorvariablen \\(X\\) beschreibt und \\(\\varepsilon\\) ein Fehlerterm ist, der unabhängig von \\(X\\) ist und Mittelwert \\(E(\\varepsilon)=0\\) Null hat. Daraus ergibt sich folgender Zusammenhang zwischen der allgemeinen Regressionsfunktion \\(f\\) und dem bedingten Mittelwert von \\(Y\\) gegeben \\(X\\): \\[ E(Y|X)=f(X) \\] Die Funktion \\(f\\) beschreibt also den bedingten Mittelwert von \\(Y\\) gegeben \\(X\\). Ziel ist es nun, die Regressionsfunktion \\(f\\) aus den Daten zu schätzen (lernen). Achtung: Die Annahme der Unabhängigkeit zwischen \\(\\varepsilon\\) und \\(X\\) kann in der Praxis verletzt sein. Die Verletzung dieser Unabhängigkeitsannahme erlaubt insbesondere keine kausale Interpretation der Ergebnisse, daher betrachtet die Literatur zur Kausalinferenz viele Möglichkeiten diese Unabhängigkeitsannahme durch eine weniger strikte Annahmen zu ersetzen. In der Literatur zur prädiktiven Inferenzen wird eine Verletzung der Unabhängigkeitsannahme weniger kritisch gesehen, da eine Prädiktion trotz verletzter Unabhängigkeitsannahme sehr gut sein kann. Eine schöne und gut lesbare Übersicht zu den Unterschieden zwischen der Kausalinferenz und der prädiktiven Inferenzen findet man, z.B., im Artikel To Explain or To Predict? (Shmueli 2010). Abbildung 2.3 zeigt ein Beispiel von \\(50\\) simulierten Daten (künstlich erzeugte Fake-Daten). Der Plot legt nahe, dass man das Einkommen mit Hilfe der Ausbildungsjahre vorhersagen kann. Normalerweise ist die wahre Funktion \\(f\\), welche die Verbindung zwischen \\(Y\\) und \\(X\\) beschreibt, unbekannt und muss aus den Daten geschätzt werden. Da es sich hier um simulierte Daten handelt, können wir den Graph der Funktion \\(f\\) als blaue Linie plotten. Einige der \\(50\\) Beobachtungspunkte \\((X,Y)\\) liegen über der Regressionsfunktion \\(f(X)\\), andere darunter. Im Großen und Ganzen haben die Fehlerterme einen Mittelwert von Null. Abbildung 2.3: Simulierte (künstlich erzeugte) Daten zur Veranschaulichung einer allgemeinen, univariaten Regressionsbeziehung. Abbildung 2.4 zeigt ein simuliertes Beispiel einer allgemeinen, bivariaten Regressionsbeziehung \\[ Y=f(X)+\\varepsilon\\quad\\text{mit}\\quad X=(X_1,X_2). \\] Abbildung 2.4: Veranschaulichung einer allgemeinen, bivariaten Regressionsbeziehung. 2.1.1 Der Prädiktionsfehler (zwischen \\(Y\\) und \\(\\hat{Y}\\)) In vielen Datenproblemen sind zwar die Prädiktorvariablen \\(X\\) bekannt (z.B. Gewicht, PS und Hubraum eines neuen Autos), aber die dazugehörige Zielvariable \\(Y\\) ist unbekannt. Da sich der Fehlerterm zu Null mittelt, lässt sich in solch einem Fall das unbekannte \\(Y\\) durch \\[ \\hat{Y}=\\hat{f}(X) \\] vorhersagen, wobei \\(\\hat{f}\\) für unsere Schätzung von \\(f\\) steht und \\(\\hat{Y}\\) die Vorhersage von \\(Y\\) für gegebene Prädiktorvariablen \\(X\\) ist. Die Genauigkeit der Vorhersage von \\(\\hat{Y}\\) für \\(Y\\) hängt von zwei verschiedenen Prädiktionsfehlergrößen ab: Reduzierbarer Prädiktionsfehler aufgrund des Schätzfehlers in \\(\\hat{f}\\). Eine genauere Schätzung kann diesen Fehler reduzieren. Nicht reduzierbarer Prädiktionsfehler aufgrund des Fehlerterms \\(\\varepsilon\\). Das ist der Fehler, den wir selbst bei perfekter Schätzung von \\(f\\) nicht reduzieren können. Der nicht reduzierbare Fehler \\(\\varepsilon\\) enthält alle nicht messbaren und nicht gemessenen Variablen, die ebenfalls einen Einfluss auf \\(Y\\) haben. Und da wir diese Variablen nicht messen können, können wir sie auch nicht verwenden, um \\(f\\) zu schätzen. Sei nun \\(\\hat{f}\\) eine gegebene Schätzung von \\(f\\) und seien \\(X\\) gegeben Werte der Prädiktorvariablen welche die Vorhersage \\(\\hat{Y}=\\hat{f}(X)\\) ergeben. Nehmen wir nun für einen Moment an, dass \\(\\hat{f}\\) und \\(X\\) gegeben und fest (also nicht zufällig) sind, dann \\[\\begin{align*} E\\left[(Y-\\hat{Y})^2\\right] &amp;=E\\Big[(\\overbrace{f(X)+\\varepsilon}^{=Y} - \\overbrace{\\hat{f}(X)}^{=\\hat{Y}})^2\\Big]\\\\ &amp;=E\\left[\\left((f(X)-\\hat{f}(X)\\right)^2+2\\left((f(X)-\\hat{f}(X)\\right)\\varepsilon+\\varepsilon^2\\right]\\\\ &amp;=\\underbrace{\\left((f(X)-\\hat{f}(X)\\right)^2}_{\\text{reduzierbar}}+\\underbrace{\\operatorname{Var}(\\varepsilon)}_{\\text{nicht reduzierbar}} \\end{align*}\\] Der mittlere quadratische Prädiktionsfehler \\(E\\left[(Y-\\hat{Y})^2\\right]\\) lässt sich also in eine reduzierbare und eine nicht reduzierbare Fehlerkomponente zerlegen. Literatur "],["2.2-das-multivariate-lineare-regressionsmodell.html", "2.2 Das multivariate lineare Regressionsmodell", " 2.2 Das multivariate lineare Regressionsmodell Um die allgemeine Regressionsfunktion \\(f(X)=E(Y|X)\\) mit Hilfe der Daten zu schätzen (lernen), gibt es sehr viele verschiedenen Möglichkeiten. Eine der erfolgreichsten und am häufigsten verwendete Möglichkeit ist das multivariaten linear Regressionsmodell. Dieses Modell ist die strukturelle Modellannahme, dass sich die unbekannte Regressionsfunktion \\(f\\) als lineare Funktion (linear in den Modellparametern \\(\\beta_0, \\beta_1, \\dots, \\beta_p\\)) schreiben lässt: \\[ f(X)=\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p. \\] Unter dieser Modellannahme wird das allgemeine Regressionsmodell \\(Y=f(X)+\\varepsilon\\) zum multivariaten (multiplen) linearen Regressionsmodell \\[\\begin{align*} Y=\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p+\\varepsilon. \\end{align*}\\] Zusammen mit der Annahme, dass \\(\\varepsilon\\) unabhängig von \\(X\\) ist, und dass \\(E(\\varepsilon)=0\\), können wir mit dieser Modellannahme den unbekannten bedingten Mittelwert \\(E(Y|X)=f(X)\\) vereinfacht schreiben als \\[\\begin{align*} E(Y|X)=\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p. \\end{align*}\\] Vorteile des multivariaten linearen Regressionsmodells: Anstatt eine gänzlich unbekannte Funktion \\(f\\) schätzen (lernen) zu müssen, muss man lediglich die unbekannten Parameterwerte \\(\\beta_0, \\beta_1, \\dots, \\beta_p\\) schätzen. Die Modellstruktur ist keine Black Box, sondern gibt Aufschluss über die assoziativen Zusammenhänge zwischen den Prädiktorvariablen und der Zielvariablen. Die lineare Modellstruktur ist extrem flexibel, da Transformationen der Prädiktorvariablen grundsätzlich erlaubt sind. Gerade die große Flexibilität linearer Modelle werden wir nutzten müssen, um die nicht linearen Zusammenhänge zwischen den Prädiktorvariablen und der Zielvariablen in unserem Benzinverbrauchsbeispiel berücksichtigen zu können (siehe Abbildung 2.2). 2.2.1 Schätzung Wir wollen nun diejenige Funktion \\[ \\hat{f}(X)=\\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\dots + \\hat{\\beta}_p X_p \\] finden, sodass \\(Y\\approx \\hat{f}(X)\\) für alle Datenpunkte \\((Y,X)\\). Zur Berechnung von \\(\\hat{f}\\) können wir die beobachteten Daten als Trainingsdaten verwenden: \\[ \\left\\{(x_1,y_1),(x_2,y_2),\\dots,(x_n,y_n)\\right\\}\\quad\\text{wobei}\\quad x_i=(x_{i1},x_{i2},\\dots,x_{ip})^T. \\] Im Folgenden werden wir oft die Notation \\[x_{ij},\\quad i=1,\\dots,n,\\quad j=1,\\dots,p\\] verwenden, um die \\(j\\)te Prädiktorvariable der \\(i\\)ten Beobachtung zu bezeichnen. Der Laufindex \\(j=1,\\dots,p\\) repräsentiert die einzelnen Prädiktorvariablen (z.B. Verbrauch, Gewicht, Pferdestärken, und Hubraum im Auto_df Datensatz) und der Laufindex \\(i=1,\\dots,n\\) repräsentiert die einzelnen Beobachtungen (z.B. gespeichert als Zeilen im Auto_df Datensatz). Idee: Die Trainingsdaten \\(\\left\\{(x_1,y_1),(x_2,y_2),\\dots,(x_n,y_n)\\right\\}\\) enthalten Information zum unbekannten Regressionsmodell \\(f\\), da (so die Grundidee) die Daten von eben diesem Modell erzeugt wurden. Ziel ist also die unbekannte Regressionsfunktion \\(f\\) mit Hilfe der Trainingsdaten zu schätzen (erlernen). Für jede mögliche Schätzung \\(\\hat{f}\\) von \\(f\\) können wir die beobachteten Werte der Zielvariablen \\(y_i\\) mit den vorhergesagten Werten \\[ \\hat{y}_i=\\hat{f}(x_i)=\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i1} + \\hat{\\beta}_2 x_{i2} + \\dots + \\hat{\\beta}_p x_{ip} \\] vergleichen, indem wir die Residuen \\[ e_i = y_i-\\hat{y}_i\\quad i=1,\\dots,n \\] betrachten. Die gängigste Methode zur Schätzung der unbekannten Modellparameter \\(\\beta_0,\\beta_1,\\dots,\\beta_p\\) ist die Methode der kleinsten Quadrate. Wir definieren die Residuenquadratsumme RSS (Residual Sum of Squares) als: \\[ \\operatorname{RSS}=e_1^2+e_2^2+\\dots +e_n^2 \\] oder äquivalent als \\[ \\operatorname{RSS}= (y_1-\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{11} + \\dots + \\hat{\\beta}_p x_{1p})^2 + \\dots + (y_n-\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{n1} + \\dots + \\hat{\\beta}_p x_{np})^2 \\] Die Methode der kleinsten Quadrate bestimmt die Parameterschätzungen \\(\\hat{\\beta}=(\\hat{\\beta}_0,\\hat{\\beta}_1,\\dots,\\hat{\\beta}_p)^T\\) durch Minimierung der Residuenquadratsumme RSS. Nach ein paar Rechnungen kann man zeigen, dass \\[\\begin{align*} \\left( \\begin{matrix} \\hat{\\beta}_0\\\\ \\hat{\\beta}_1\\\\ \\vdots\\\\ \\hat{\\beta}_p \\end{matrix} \\right)= \\left( \\left(\\begin{matrix} 1&amp;x_{11}&amp;\\dots &amp; x_{1p}\\\\ \\vdots&amp;&amp;\\ddots &amp; \\vdots\\\\ 1&amp;x_{n1}&amp;\\dots &amp; x_{np}\\\\ \\end{matrix}\\right)^T \\left(\\begin{matrix} 1&amp;x_{11}&amp;\\dots &amp; x_{1p}\\\\ \\vdots&amp;&amp;\\ddots &amp; \\vdots\\\\ 1&amp;x_{n1}&amp;\\dots &amp; x_{np}\\\\ \\end{matrix}\\right) \\right)^{-1} \\left(\\begin{matrix} 1&amp;x_{11}&amp;\\dots &amp; x_{1p}\\\\ \\vdots&amp;&amp;\\ddots &amp; \\vdots\\\\ 1&amp;x_{n1}&amp;\\dots &amp; x_{np}\\\\ \\end{matrix}\\right)^T \\left( \\begin{matrix} Y_1\\\\ \\vdots\\\\ Y_n \\end{matrix} \\right) \\end{align*}\\] 2.2.2 Polynomregression Die Polynomregression ist eine Möglichkeit, die nicht linearen Beziehungen zwischen der Zielvariablen und den Prädiktorvariablen in unserem Benzinverbrauchsproblem (siehe Abbildung 2.2) berücksichtigen zu können. So kann, zum Beispiel, der nicht lineare Zusammenhang zwischen Verbrauch und Leistung PS sehr flexibel als Polynomfunktion modelliert werden: \\[ \\texttt{Verbrauch}=\\beta_0 + \\beta_1 \\texttt{Ps} + \\beta_2 \\texttt{PS}^2 + \\dots + \\beta_p \\texttt{PS}^p \\] Je höher der Grad \\(p\\) des Polynoms, desto flexibler ist ein Polynomregressionsmodell und ermöglicht so auch die Modellierung nicht linearen Zusammenhänge. Das Polynomregressionsmodell ist jedoch für alle Polynomgrade \\(p\\) ein (multivariates) lineares Regressionsmodell, denn es ist linear bezüglich der Modellparameter \\(\\beta_0, \\beta_1, \\dots, \\beta_p\\). ## Polynom Regressionen polreg_1 &lt;- lm(Verbrauch ~ poly(PS, degree = 1, raw=TRUE), data = Auto_df) polreg_2 &lt;- lm(Verbrauch ~ poly(PS, degree = 2, raw=TRUE), data = Auto_df) polreg_5 &lt;- lm(Verbrauch ~ poly(PS, degree = 5, raw=TRUE), data = Auto_df) ## Data-Frame zum Abspeichern der Prädiktionen plot_df &lt;- tibble(&quot;PS&quot; = seq(45, 250, len=50)) ## Abspeichern der Prädiktionen plot_df$fit_1 &lt;- predict(polreg_1, newdata = plot_df) plot_df$fit_2 &lt;- predict(polreg_2, newdata = plot_df) plot_df$fit_5 &lt;- predict(polreg_5, newdata = plot_df) ## Ploten plot(Verbrauch ~ PS, data = Auto_df, ylim=c(2,20), xlab=&quot;Leistung (PS)&quot;, pch=21, col=&quot;gray&quot;, bg=&quot;gray&quot;, cex=1.5) with(plot_df, lines(x = PS, y = fit_1, lwd=2, col=&quot;orange&quot;)) with(plot_df, lines(x = PS, y = fit_2, lwd=2, col=&quot;blue&quot;)) with(plot_df, lines(x = PS, y = fit_5, lwd=2, col=&quot;darkgreen&quot;)) legend(&quot;topright&quot;, lty=c(NA,1,1,1), pch=c(21,NA,NA,NA), col=c(&quot;gray&quot;,&quot;orange&quot;,&quot;blue&quot;,&quot;darkgreen&quot;), pt.bg=&quot;gray&quot;, pt.cex=1.5, legend=c(&quot;Datenpunkte&quot;, &quot;Grad 1&quot;, &quot;Grad 2&quot;, &quot;Grad 5&quot;), bty=&quot;n&quot;) Abbildung 2.5: Polynom Regression bei verschiedenen Polynomgraden \\(p\\). Überanpassung Zusätzlich zur Wahl der Modellparameter \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_p\\) besteht hier nun das Problem der Wahl des Grades \\(p\\) des Polynoms als weiteren Modellparameter \\[ y_i=\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i1} + \\hat{\\beta}_2 x_{i2}^2 + \\dots + \\hat{\\beta}_p x_{ip}^p + e_i \\] Wenn man jedoch versucht, alle Modellparameter (also \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_p\\) und \\(p\\)) durch Minimieren der Trainingsdaten-RSS \\[ \\operatorname{RSS}\\equiv\\operatorname{RSS}(\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_p,p)=e_1^2 + e_2^2 + \\dots + e_n^2 \\] zu schätzen, so ergibt sich ein Problem das als Überanpassung (Overfitting) bekannt ist (siehe Abbildung 2.6). Das Polynomregressionsmodell ist so flexibel, dass es den einzelnen Trainingsdaten \\((x_i,y_i)\\) folgen kann. Eine Überangepassung an die Trainingsdaten führt jedoch notwendigerweise zu einer Verschlechterung der Vorhersagegüte bezüglich neuer Daten. Abbildung 2.6: Polynom Regression und die Wahl des Polynomgrades \\(p\\) durch Minimierung der Trainingsdaten-RSS. (Eine schlechte Idee). "],["2.3-modellauswahl.html", "2.3 Modellauswahl", " 2.3 Modellauswahl Maschinelles Lernen versus Strukturelle Modelle Das oben veranschaulichte Problem der Überanpassung (Overfitting) ist eng damit verbunden, dass wir hier ein sehr flexibles Regressionsmodell (Polynomregression) betrachten. Viele der möglichen Polynomfunktionen sind unsinnig, da sie nicht die strukturellen Einschränkungen des betrachteten Datenproblems berücksichtigen. Falls ein gesichertes Wissen zu den zugrundeliegenden, strukturellen Zusammenhängen zwischen der Zielvariable \\(Y\\) und den Prädiktorvariablen \\(X\\) existiert, sollte man diese strukturellen Zusammenhängen auch im statistischen Modell berücksichtigen. (Immer mit den Expert*Innen des Faches sprechen!) Im besten Falle gibt es ein strukturelles Modell zu den systematischen Zusammenhängen \\(f\\) zwischen \\(Y\\) und \\(X\\), welches genügend Einschränkungen bietet, sodass alle unsinnigen Modellierungen vermieden werden können. In solchen Idealfällen führt die Minimierung der Trainingsdaten-RSS zu keinem Problem der Überanpassung. Falls jedoch kein (vertrauenswürdiges) strukturelles Modell vorliegt, ist die Verwendung von sehr flexiblen Regressionsmodellen wie der Polynomregression eine grundsätzlich sehr gute Idee, da wir so, ohne große Einschränkungen, nach den unbekannten richtigen Zusammenhängen \\(f\\) suchen können. Dies ist der Ansatz des maschinellen Lernens und die Polynomregression mit unbekanntem Polynomgrad \\(p\\) ist lediglich eine von sehr vielen Methoden, welche im Kontext des maschinellen Lernens verwendet werden. Fazit: Methoden des maschinellen Lernens sind typischerweise sehr flexibel und bauen nicht auf strukturellen Modellen auf. Daher benötigen diese Methoden spezielle Verfahren der Modellauswahl, um eine Überanpassung an die Trainingsdaten zu vermeiden. Richtig angewandt, können Methoden des maschinellen Lernens unbekannte Zusammenhänge richtig erkennen. 2.3.1 Die Validierungsdaten-Methode Da die Minimierung der Trainingsdaten-RSS schnell zu einem Problem der Überanpassung führt, benötigen wir eine alternative Methode, um die Güte des geschätzten Modells zu prüfen. Die einfachste Idee ist dabei die beobachteten Daten in einen Satz von Trainingsdaten \\[ \\text{Trainingsdaten}=\\left\\{(x_{1}^{Train},y_{1}^{Train}), (x_{2}^{Train},y_{2}^{Train}),\\dots,(x_{n_{Train}}^{Train},y_{n_{Train}}^{Train})\\right\\} \\] und einen separaten (disjunkten) Satz von Validierungsdaten \\[ \\text{Validierungsdaten}=\\left\\{(x_{1}^{Valid},y_{1}^{Valid}), (x_{2}^{Valid},y_{2}^{Valid}),\\dots,(x_{n_{Valid}}^{Valid},y_{n_{Valid}}^{Valid})\\right\\} \\] zu teilen mit \\(n=n_{Train} + n_{Valid}\\) \\(\\text{Trainingsdaten}\\cap \\text{Validierungsdaten} = \\emptyset\\) Folgender Code-Schnipsel ermöglicht solch eine (zufällige) Aufteilung der Daten in Trainings- und Validierungsdaten: n &lt;- nrow(Auto_df) # Stichprobenumfang n_Train &lt;- 200 # Stichprobenumfang der Trainingsdaten n_Valid &lt;- n - n_Train # Stichprobenumfang der Validierungsdaten ## Index-Mengen zur Auswahl der ## Trainings- und Validierungsdaten I_Train &lt;- sample(x = 1:n, size = n_Train, replace = FALSE) I_Valid &lt;- c(1:n)[-I_Train] ## Trainingsdaten Auto_Train_df &lt;- Auto_df[I_Train, ] ## Validierungsdaten Auto_Valid_df &lt;- Auto_df[I_Valid, ] Obschon die Validierungsdaten-Methode auf alle Regressionsmodelle angewandt werden kann, veranschaulichen wir im Folgenden die Methode anhand der Polynomregression. Die Aufteilung der Daten in Trainings- und Validierungsdaten ermöglicht uns nun ein zweistufiges Verfahren: Schritt 1: Mit Hilfe der Trainingsdaten wird das Polynomregressionsmodell geschätzt: \\[\\begin{align*} y^{Train}_i %&amp;=\\hat{f}^{Train}_p(x_i^{Train}) + e_i^{Train}\\\\ &amp;=\\hat{\\beta}_0^{Train} + \\hat{\\beta}_1^{Train} x_{i}^{Train} + \\hat{\\beta}_2^{Train} (x_{i}^{Train})^2 + \\dots + \\hat{\\beta}_p^{Train} (x_{i}^{Train})^p + e_i^{Train} \\end{align*}\\] Code-Schnipsel Beispiel: Train_polreg &lt;- lm(Verbrauch ~ poly(PS, degree = p, raw=TRUE), data = Auto_Train_df) Schritt 2: Mit Hilfe der Validierungsdaten wird das geschätzte Polynomregressionsmodell validiert: \\[\\begin{align*} \\hat{y}^{Valid}_i %&amp;=\\hat{f}_p^{Train}(x_i^{Valid})+ e_i^{Valid}\\\\ &amp;=\\hat{\\beta}_0 + \\hat{\\beta}_1^{Train} x_{i}^{Valid} + \\hat{\\beta}_2^{Train} (x_{i}^{Valid})^2 + \\dots + \\hat{\\beta}_p^{Train} (x_{i}^{Valid})^p, \\end{align*}\\] indem man den mittleren quadratischen Prädiktionsfehler (Mean Squared Prediction Error MSPE) berechnet: \\[\\begin{align*} \\text{MSPE} &amp;=\\frac{1}{n_{Valid}}\\text{RSS}_{Valid}\\\\ &amp;=\\frac{1}{n_{Valid}}\\left((y_1^{Valid} - \\hat{y}_1^{Valid})^2 +\\dots + (y_{n_{Valid}}^{Valid} - \\hat{y}_{n_{Valid}}^{Valid})^2\\right) \\end{align*}\\] Code-Schnipsel Beispiel: y_fit_Valid &lt;- predict(Train_polreg, newdata = Auto_Valid_df) RSS_Valid &lt;- sum( (Auto_Valid_df$Verbrauch - y_fit_Valid)^2 ) MSPE &lt;- RSS_Valid / n_Valid Man wiederholt obige Schritte für eine Auswahl von verschiedenen Polynomgraden \\(p=1,2,\\dots,p_{\\max}\\), z.B. \\(p_{\\max}=10\\), und berechnet für jeden dieser Fälle den \\(\\operatorname{MSPE}\\), also: \\[ \\operatorname{MSPE}\\equiv\\operatorname{MSPE}(\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_p,p),\\quad\\text{für jedes}\\quad p=1,2,\\dots,p_{\\max} \\] Der \\(\\operatorname{MSPE}\\) ist eine Schätzung des wahren, unbekannten mittleren quadratischen Prädiktionsfehlers \\(E\\left[(Y-\\hat{Y})^2\\right]\\), \\[ \\operatorname{MSPE}(\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_p,p)\\approx E\\left[(Y-\\hat{Y})^2\\right]. \\] Die Minimierung des \\(\\operatorname{MSPE}\\) über verschiedene Werte des Polynomgrades \\(p=1,2,\\dots\\) erlaubt es uns den reduzierbaren Prädiktions-Fehler der Polynomregression zu minimieren. Folgender R-Code verbindet nun alle Schritte und berechnet den \\(\\operatorname{MSPE}\\) für verschiedene Werte des Polynomgrades \\(p\\). Dasjenige Modell, welches den \\(\\operatorname{MSPE}\\) minimiert, ist laut der Daten das beste Prädiktionsmodell. set.seed(31) ## n &lt;- nrow(Auto_df) # Stichprobenumfang n_Train &lt;- 200 # Stichprobenumfang der Trainingsdaten n_Valid &lt;-n - n_Train # Stichprobenumfang der Validierungsdaten ## Index-Mengen zur Auswahl der ## Trainings- und Validierungsdaten I_Train &lt;- sample(x = 1:n, size = n_Train, replace = FALSE) I_Valid &lt;- c(1:n)[-I_Train] ## Trainingsdaten Auto_Train_df &lt;- Auto_df[I_Train, ] ## Validierungsdaten Auto_Valid_df &lt;- Auto_df[I_Valid, ] p_max &lt;- 6 MSPE &lt;- numeric(p_max) fit_plot &lt;- matrix(NA, 50, p_max) for(p in 1:p_max){ ## Schritt 1 Train_polreg &lt;- lm(Verbrauch ~ poly(PS, degree = p, raw=TRUE), data = Auto_Train_df) ## Schritt 2 y_fit_Valid &lt;- predict(Train_polreg, newdata = Auto_Valid_df) RSS_Valid &lt;- sum( (Auto_Valid_df$Verbrauch - y_fit_Valid)^2 ) MSPE[p] &lt;- RSS_Valid / n_Valid ## Daten für&#39;s plotten fit_plot[,p] &lt;- predict(Train_polreg, newdata = plot_df) } Abbildung 2.7: Polynom Regression und die Wahl des Polynomgrades \\(p\\) durch Minimierung des mittleren quadratischen Prädiktionsfehler MSPE. Achtung: Auch eine Modellauswahl ist fehlerhaft und stellt lediglich eine Schätzung (mit Schätzfehlern) des besten Prädiktionsmodelles innerhalb der betrachteten Klasse von Prädiktionsmodellen (hier Polynomregressionen) dar. Abbildung 2.8 zeigt jedoch ein Problem der Validierungsdaten-Methode. Die Trainingsdaten und die Validierungsdaten haben kleinere Stichprobenumfänge (\\(n_{Train}&lt;n\\) und \\(n_{Valid}&lt;n\\)) was zu einer erhöhten Schätzgenauigkeit in der MSPE-Schätzung führt. Abbildung 2.8: Zehn verschiedene MSPE-Berechnungen basierend auf zehn verschiedenen, zufälligen Aufteilungen der Daten in Trainings- und Validierungsdaten. 2.3.2 k-Fache Kreuzvalidierung Die \\(k\\)-fache (z.B. \\(k=5\\) oder \\(k=10\\)) Kreuzvalidierung ist eine Vorgehensweise zur Bewertung der Leistung einer Schätzprozedur (Algorithmus) im Kontext des maschinellen Lernens. Als Schätzprozedur verwenden wir wieder das Beispiel der Polynomregression mit unbekanntem Polynomgrad \\(p\\), welcher zusammen mit den Modellparametern \\(\\beta_0,\\beta_1,\\dots,\\beta_p\\) aus den Daten erlernt werden muss. Die \\(k\\)-fache Kreuzvalidierung stellt eine Verbesserung der Validierungsdaten-Methode dar, da sie faktisch die Stichprobenumfänge in den Trainingsdaten und Validierungsdaten erhöht. Wie bei der Validierungsdaten-Methode wird der Datensatz in Trainings- und Validierungsdaten aufgeteilt – jedoch \\(k\\)-fach. Abbildung 2.9 zeigt ein Beispiel der Datenaufteilung bei der \\(5\\)-fachen Kreuzvalidierung. Abbildung 2.9: Datenaufteilung in Trainings- und Validierungsdaten bei der \\(5\\)-fachen Kreuzvalidierung. Folgender Code-Schnipsel ermöglicht eine (zufällige) Aufteilung der Daten in \\(k\\) verschiedene Trainings- und Validierungsdaten: n &lt;- nrow(Auto_df) # Stichprobenumfang k &lt;- 5 # 5-fache Kreuzvalidierung ## Index zur Auswahl k verschiedener ## Trainings- und Validierungsdaten: folds &lt;- sample(x = 1:k, size = n, replace=TRUE) ## Trainingsdaten im j-ten (j=1,2,...,k) Durchgang Auto_df[folds != j,] ## Validierungsdaten im j-ten (j=1,2,...,k) Durchgang Auto_df[folds == j,] Für jede der \\(k\\) Datenaufteilungen wird der \\(\\operatorname{MSPE}\\) berechnet. Der Mittelwert dieser MSPE-Werte wird häufig als \\(\\operatorname{CV}_{(k)}\\) Wert (crossvalidation score) bezeichnet \\[ \\operatorname{CV}_{(k)}=\\frac{1}{k}\\sum_{j=1}^k\\operatorname{MSPE}_j \\] Der \\(\\operatorname{CV}_{(k)}\\)-Wert stellt eine im Vergleich zur Validierungsdaten-Methode verbesserte Schätzung des unbekannten mittleren quadratischen Pädiktionsfehlers \\(\\operatorname{CV}_{(k)}\\approx E[(Y-\\hat{Y})^2]\\) dar. Die Modellauswahl folgt also auch hier mittels Minimierung des \\(\\operatorname{CV}_{(k)}\\)-Wertes über die verschiedene Werte des Polynomgrades \\(p=1,2,\\dots\\). Wahl von \\(k\\): In der Praxis haben sich die Werte \\(k=5\\) und \\(k=10\\) etabliert, da diese Größenordnunen einen guten Kompromiss zwischen der Varianz und der Verzerrung des Schätzers \\(\\operatorname{CV}_{(k)}\\) für \\(E[(Y-\\hat{Y})^2]\\) darstellen. "],["2.4-anwendung-vorhersage-des-benzinverbrauchs.html", "2.4 Anwendung: Vorhersage des Benzinverbrauchs", " 2.4 Anwendung: Vorhersage des Benzinverbrauchs Nun haben wir das Werkzeug, um die nicht linearen Zusammenhänge zwischen der Zielvariable \\(Y=\\)Verbrauch und den Prädiktorvariablen \\(G=\\)Gewicht, \\(P=\\)PS und \\(H=\\)Hubraum im Datensatz Auto_df zu berücksichtigen (siehe Abbildung 2.2) und allein mit Hilfe der Daten zu erlernen. Wir folgen hier der Herangehensweise des maschinellen Lernens und lassen die Daten für sich selbst sprechen. Da Abbildung 2.2 sehr ähnliche Zusammenhänge zwischen der Zielvariable \\(Y=\\)Verbrauch und den Prädiktorvariablen \\(G=\\)Gewicht, \\(P=\\)PS und \\(H=\\)Hubraum vermuten lässt, betrachten wir zunächst ein vereinfachtest Polynomregressionsmodell, bei dem für alle Prädiktorvariablen der gleiche Polynomgrad \\(p\\) verwendet wird. \\[\\begin{align*} Y_i = \\beta_0 + \\notag &amp; \\beta^G_{1} G_i + \\beta^G_{2} G_i^2 + \\dots + \\beta^G_{p} G_i^p + \\\\ &amp; \\beta^P_{1} P_i + \\beta^P_{2} P_i^2 + \\dots + \\beta^P_{p} P_i^p + \\\\ &amp; \\beta^H_{1} H_i + \\beta^H_{2} H_i^2 + \\dots + \\beta^H_{p} H_i^p + \\varepsilon_i \\end{align*}\\] Folgender R-Code (Algorithmus) erlernt aus den Daten, mit Hilfe der \\(5\\)-fachen Kreuzvalidierung \\(\\operatorname{CV}_{(5)}\\approx E[(Y-\\hat{Y})^2]\\), den optimalen Polynomgrad \\(p\\). set.seed(8) # Seed für den Zufallsgenerator n &lt;- nrow(Auto_df) # Stichprobenumfang k &lt;- 5 # 5-fache Kreuzvalidierung p_max &lt;- 5 # Maximaler Polynomgrad folds &lt;- sample(x = 1:k, size = n, replace=TRUE) ## Container für die MSPE-Werte ## für alle j=1,...,k Kreuzvalidierungen und ## für alle p=1,...,p_max Polynomgrade MSPE &lt;- matrix(NA, nrow = k, ncol = p_max, dimnames=list(NULL, paste0(&quot;p=&quot;,1:p_max))) for(p in 1:p_max){ for(j in 1:k){ ## Modelschätzung auf Basis j-ten Traininsdaten Auto_df[folds != j,] poly_fit &lt;- lm(Verbrauch ~ poly(Gewicht, degree = p, raw = TRUE) + poly(PS, degree = p, raw = TRUE) + poly(Hubraum, degree = p, raw = TRUE), data=Auto_df[folds != j,]) ## Prädiktion auf Basis j-ten Validierungsdaten Auto_df[folds == j,] pred &lt;- predict(poly_fit, newdata = Auto_df[folds == j,]) ## MSPE[j,p] &lt;- mean( (Auto_df$Verbrauch[folds==j] - pred)^2 ) } } ## CV-Wert für alle p=1,...,p_max Polynomgrade CV_k &lt;- colMeans(MSPE) ## Plotten plot(y = CV_k, x = 1:length(CV_k), pch=21, col=&quot;black&quot;, bg=&quot;black&quot;, type=&#39;b&#39;, xlab=&quot;Polynomgrad p&quot;, ylab=expression(CV[(5)]), log=&quot;y&quot;) points(y = CV_k[which.min(CV_k)], x = c(1:length(CV_k))[which.min(CV_k)], col = &quot;red&quot;, bg = &quot;red&quot;, pch = 21) Auch der \\(5\\)-fache Kreuzvalidierungswert \\(\\operatorname{CV}_{(5)}\\) ist lediglich eine zufallsbehaftete Schätzung des unbekannten mittleren quadratischen Prädiktionsfehlers \\(E[(Y-\\hat{Y})^2]\\). Um eine Idee von der Präzision und Stabilität der Modellauswahl mittels der Minimierung von \\(\\operatorname{CV}_{(5)}\\) zu bekommen, können wir die zufälligen, \\(5\\)-fachen Aufteilungen der Daten in Trainins- und Validierungsdaten wiederholen und den Effekt alternativer Datenaufteilungen betrachten. Abbildung 2.10 zeigt, dass die Minimierung des Kreuzvalidierungswertes \\(\\operatorname{CV}_{(5)}\\) auch in Wiederholungen häufig das Modell mit Polynomgrad \\(p=2\\) auswählt. Der Polynomgrad \\(p=2\\) scheint also eine vertauenswürde Modellauswahl darzustellen. Abbildung 2.10: Zehn verschiedene \\(\\operatorname{CV}_{(k)}\\)-Berechnungen basierend auf zehn verschiedenen, zufälligen Wiederholungen der \\(5\\)-fachen Kreuzvalidierung. Das Polynomregressionsmodell mit \\(p=2\\) stellt also ein gutes Prädiktionsmodell dar. Wir verwenden nun dieses Modell, um nach auffälligen Unterschiedenen in den herstellerseitigen Verbrauchsangaben \\(y_i\\) und unseren Prädiktionen zu suchen. Gerade stark negative Residuen \\(y_i-\\hat{y}_i\\) sind verdächtig, da es auf eine Schönung der Verbrauchsangaben hindeuten könnte. Folgender R-Code schätzt zunächst das Polynomregressionsmodell mit \\(p=2\\), berechnet dann die Residuen \\(y_i-\\hat{y}_i\\) und veranschaulicht die größte negative Abweichung in Abbildung 2.11. p &lt;- 2 poly_fit &lt;- lm(Verbrauch ~ poly(Gewicht, degree = p, raw = TRUE) + poly(PS, degree = p, raw = TRUE) + poly(Hubraum, degree = p, raw = TRUE), data=Auto_df) par(mar=c(5.1, 5.1, 4.1, 2.1)) plot(y = resid(poly_fit), x = fitted(poly_fit), ylab = expression(&quot;Residuen:&quot;~y[i] - hat(y)[i]), xlab = expression(&quot;Prädiktionen:&quot;~hat(y)[i]), main=&quot;Größte negative Abweichung der Verbrauchsangabe&quot;) slct &lt;- order(abs(resid(poly_fit)), decreasing = TRUE)[4] points(y = resid(poly_fit)[slct], x = fitted(poly_fit)[slct], col = &quot;red&quot;, bg = &quot;red&quot;, pch = 21) text(y = resid(poly_fit)[slct], x = fitted(poly_fit)[slct], labels = &quot;Mazda RX-3 (1973)&quot;, pos = 2) Abbildung 2.11: Polynomregression im Anwendungsbeispiel zum Benzinverbrauch. Die größte negative Abweichung ist der Mazda RX-3 von 1973. par(mar=c(5.1, 4.1, 4.1, 2.1)) Wir haben hier tatsächlich einen besonderen Fall gefunden. Der Mazda RX-3 (1973) (Abbildung 2.12) lief mit einem ungewöhnlich sparsamen Wankelmotor. Dieser Motor war so ungewöhnlich, dass es vielerlei Streitigkeiten um die Verbrauchsangaben gab. Abbildung 2.12: Mazda RX-3 hatte einen Wankelmotor. "],["2.5-ende-1.html", "2.5 Ende", " 2.5 Ende Abbildung 2.13: Curve-Fitting nach (xkcd)[https://xkcd.com/2048/]. "],["literatur.html", "Literatur", " Literatur "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
