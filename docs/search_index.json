[
["index.html", "Computational Statistics Informationen", " Computational Statistics Prof.¬†Dr.¬†Dominik Liebl Informationen Dies ist das Skript zur Vorlesung Computational Statistik (B.Sc. Informatik &amp; Data Science) Vorlesungszeiten Wochentag Uhrzeit H√∂rsaal Dienstag 9:15-10:45 Online-Vorlesung Freitag 8:30-10:00 Online-Vorlesung RCodes Die RCodes zu den einzelnen Kapiteln k√∂nnen hier heruntergeladen werden: RCodes Leseecke Folgende frei zug√§ngliche Lehrb√ºcher enthalten Teile dieses Kurses. In den jeweiligen Kapiteln, werde ich auf die einzelnen B√ºcher verweisen. Pattern Recognition and Machine Learning (by Christopher Bishop) An Introduction to Statistical Learning, with Applications in R (by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani). Statistical Learning with Sparsity: the Lasso and Generalizations (by Trevor Hastie, Robert Tibshirani and Martin Wainwright). Elements of Statistical Learning: Data mining, Inference and Prediction (by Trevor Hastie, Robert Tibshirani and Jerome Friedman). Computer Age Statistical Inference: Algorithms, Evidence and Data Science (by Bradley Efron and Trevor Hastie) Florence Nightingale Das Logo zu diesem Skript stammt von einer Briefmarke zur Erinnerung an die Krankenschwester und inspirierende Statistikerin, Florence Nightingale. Nightingale war die Begr√ºnderin der modernen westlichen Krankenpflege und Pionierin der visuellen Datenanalyse. Sie nutzte statistische Analysen, um Missst√§nde in Kliniken zu erkennen und diese dann auch nachweislich abzustellen. Sie ist die erste Frau, die in die britische Royal Statistical Society aufgenommen wurde; sp√§ter erhielt sie auch die Ehrenmitgliedschaft der American Statistical Association. Dieses Skript ist lizenziert unter der Creative Commons Lizenz Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["1-der-expectation-maximization-em-algorithmus.html", "1 Der Expectation Maximization (EM) Algorithmus", " 1 Der Expectation Maximization (EM) Algorithmus span { display: inline-block; } Der EM Algorithmus wird h√§ufig verwendet, um komplizierte Maximum Likelihood Sch√§tzprobleme zu vereinfachen bzw. √ºberhaupt erst m√∂glich zu machen. In diesem Kapitel stellen wir den EM Algorithmus zur Sch√§tzung von Gau√üschen Mischverteilungen vor, da der EM Algorithmus hier wohl seine bekannteste Anwendung hat. Bereits die originale Arbeit zum EM Algorithmus (Dempster, Laird, und Rubin 1977) besch√§ftigt sich mit der Sch√§tzung von Gau√üschen Mischverteilungen. M√∂gliche Anwendungen von Gau√üschen Mischverteilungen: Automatisierte Videobearbeitungen: Z.B. Bildeinteilungen in Vorder- und Hintergrund. Automatisierte Erkennung von Laufstilen Generell: Auffinden von Gruppierungen (zwei oder mehr) in den Daten (Clusteranalyse). Lernziele f√ºr dieses Kapitel Sie k√∂nnen ‚Ä¶ ein Anwendungsfeld des EM Algorithmuses benennen. die Probleme der klassischen Maximum Likelihood Methode zur Sch√§tzung von Gau√üschen Mischverteilungen benennen und erkl√§utern. die Grundidee des EM Algorithmuses erl√§utern. den EM Algorithmus zur Sch√§tzung von Gau√üschen Mischverteilungen anwenden. das Grundidee der Vervollst√§ndigung der Daten durch latente Variablen erl√§utern. Begleitlekt√ºre(n) Zur Vorbereitung der Klausur ist es grunds√§tzlich aussreichend das Kursskript durchzuarbeiten - aber Lesen hat ja noch nie geschadet. Dieses Kapitel basiert haupts√§chlich auf: Kapitel 9 in Pattern Recognition and Machine Learning (Bishop 2006). Die pdf-Version des Buches ist frei erh√§ltlichen: pdf-Version Weiteren guten Lesestoff zum EM Algoithmus gibt es z.B. hier: Kapitel 8.5 in Elements of Statistical Learning: Data Mining, Inference and Prediction (Hastie, Tibshirani, und Friedman 2009). Die pdf-Version des Buches ist frei erh√§ltlichen: pdf-Version R-Pakete f√ºr diese Kapitel Folgende R-Pakete werden f√ºr dieses Kapitel ben√∂tigt: pkgs &lt;- c(&quot;tidyverse&quot;, # Die tidyverse-Pakete &quot;palmerpenguins&quot;, # Pinguin-Daten &quot;scales&quot;, # Transparente Farben: alpha() &quot;RColorBrewer&quot;, # H√ºbsche Farben &quot;mclust&quot;, # Sch√§tzung/Verwendung # Gau√üschen Mischverteilungen &quot;MASS&quot;) # Erzeugung von Zufallszahlen aus # einer multiv. Normalverteilung install.packages(pkgs) Literatur "],
["1-1-motivation-clusteranalyse-mit-hilfe-gau√üscher-mischverteilungen.html", "1.1 Motivation: Clusteranalyse mit Hilfe Gau√üscher Mischverteilungen", " 1.1 Motivation: Clusteranalyse mit Hilfe Gau√üscher Mischverteilungen Als Datenbeispiel verwendent wir die palmerpenguins Daten (Horst, Hill, und Gorman 2020). Diese Daten stammen aus Vermessungen von Pinguinpopulationen auf dem Palmer-Archipel (Antarktische Halbinsel). Pinguine sind oft schwer von einander zu unterscheiden (Abbildung 1.1). Wir werden versuchen, mit Hilfe einer Gau√üschen Mischverteilung Gruppierungen in den Pinguindaten (Flossenl√§nge) zu finden. Um solche Mischverteilungen sch√§tzen zu k√∂nnen, f√ºhren wir den EM Algorithmus ein. Abbildung 1.1: Frecher Pinguin bei der Tat. Der folgende Code-Chunck bereitet die Daten auf. Achtung: Wir haben zwar die Information zu den verschiedenen Pinguin-Arten (Penguine_Art) tun aber im Folgenden so, also ob wir diese Information nicht kennen w√ºrden. Wir wollen alleine auf Basis der Flossenl√§ngen (Penguine_Flosse) die Gruppenzugeh√∂rigkeiten per Clusteranalyse bestimmen. (Im Nachhinein k√∂nnen wir dann mit Hilfe der Daten in Penguine_Art pr√ºfen, wie gut unsere Clusteranalyse ist.) library(&quot;palmerpenguins&quot;) # Pinguin-Daten library(&quot;RColorBrewer&quot;) # H√ºbsche Farben library(&quot;scales&quot;) # F√ºr transparente Farben: alpha() col_v &lt;- RColorBrewer::brewer.pal(n = 3, name = &quot;Set2&quot;) ## Vorbereitung der Daten: Pinguine &lt;- palmerpenguins::penguins %&gt;% # Pinguin-Daten tidyr::as_tibble() %&gt;% # Datenformat: &#39;tibble&#39;-dataframe dplyr::filter(species!=&quot;Adelie&quot;) %&gt;% # Pinguin-Art &#39;Adelie&#39; l√∂schen droplevels() %&gt;% # L√∂sche das nicht mehr ben√∂tigte Adelie-Level tidyr::drop_na() %&gt;% # NAs l√∂schen dplyr::mutate(Art = species, # Variablen umbenennen Flosse = flipper_length_mm) %&gt;% dplyr::select(Art, Flosse) # Variablen ausw√§hlen ## n &lt;- nrow(Pinguine) # Stichprobenumfang ## Variable &#39;Penguine_Art&#39; aus Pinguine-Daten &quot;herausziehen&quot; Penguine_Art &lt;- dplyr::pull(Pinguine, Art) ## Variable &#39;Penguine_Flosse&#39; aus Pinguine-Daten &quot;herausziehen&quot; Penguine_Flosse &lt;- dplyr::pull(Pinguine, Flosse) ## Plot ## Histogramm: hist(x = Penguine_Flosse, freq = FALSE, xlab=&quot;Flossenl√§nge (mm)&quot;, main=&quot;Pinguine\\n(Zwei Gruppen)&quot;, col=gray(.65,.5), border=gray(.35,.5), ylim=c(0.0003, 0.039)) ## Stipchart hinzuf√ºgen: stripchart(x = Penguine_Flosse, method = &quot;jitter&quot;, jitter = .0005, at = .001, pch = 21, col=alpha(col_v[3],.5), bg=alpha(col_v[3],.5), cex=1.3, add = TRUE) Das Clusterverfahren basierend auf Gau√üschen Mischverteilungen: Gau√üsche Mischverteilung (per EM Algorithmus) sch√§tzen Die Datenpunkte wie bei der Linearen (oder Quadratischen) Diskriminanz-Analyse den Gruppen zuordnen (siehe Abbildung 1.2) Abbildung 1.2: Clusteranalyse basierend auf einer Mischverteilung mit zwei gewichteten Normalverteilungen. Abbildung 1.2 zeigt das Resultat einer Clusteranalyse basierend auf einer Mischverteilung zweier gewichteter Normalverteilungen. Cluster-Ergebnis: 95% der Pinguine konnten richtig zugeordnet werden - lediglich auf Basis ihrer Flossenl√§ngen. Mit Hilfe der folgenden R-Codes kann die obige Clusteranalyse und die Ergebnisgrafik repliziert werden: ## mclust R-Paket: ## Clusteranalyse mit Hilfe von Gau√üschen Mischmodellen suppressMessages(library(&quot;mclust&quot;)) ## Anzahl der Gruppen G &lt;- 2 ## Sch√§tzung des Gau√üschen Mischmodells (per EM Algorithmus) ## und Clusteranalyse mclust_obj &lt;- mclust::Mclust(data = Penguine_Flosse, G=G, modelNames = &quot;V&quot;, verbose = FALSE) # summary(mclust_obj) # str(mclust_obj) ## Gesch√§tzte Gruppen-Zuordnungen (Cluster-Resultat) class &lt;- mclust_obj$classification ## Anteil der korrekten Zuordnungen: # cbind(class, Penguine_Art) round(sum(class == as.numeric(Penguine_Art))/n, 2) ## Gesch√§tzte Mittelwerte mean_m &lt;- t(mclust_obj$parameters$mean) ## Gesch√§tzte Varianzen (und evtl. Kovarianzen) cov_l &lt;- list(&quot;Cov1&quot; = mclust_obj$parameters$variance$sigmasq[1], &quot;Cov2&quot; = mclust_obj$parameters$variance$sigmasq[2]) ## Gesch√§tzte Gewichte (a-priori-Wahrscheinlichkeiten) prop_v &lt;- mclust_obj$parameters$pro ## Auswerten der Gau√üsche Mischung-Dichtefunktion np &lt;- 100 # Anzahl der Auswertungspunkte xxd &lt;- seq(min(Penguine_Flosse)-3, max(Penguine_Flosse)+5, length.out = np) ## Mischungs-Dichte yyd &lt;- dnorm(xxd, mean_m[1], sqrt(cov_l[[1]]))*prop_v[1] + dnorm(xxd, mean_m[2], sqrt(cov_l[[2]]))*prop_v[2] ## Einzel-Dichten yyd1 &lt;- dnorm(xxd, mean_m[1], sqrt(cov_l[[1]]))*prop_v[1] yyd2 &lt;- dnorm(xxd, mean_m[2], sqrt(cov_l[[2]]))*prop_v[2] ## Plot hist(x = Penguine_Flosse, xlab=&quot;Flossenl√§nge (mm)&quot;, main=&quot;Pinguine\\n(Zwei Gruppen)&quot;, col=gray(.65,.5), border=gray(.35,.5), freq = FALSE, ylim=c(0, 0.04)) lines(x = xxd, y=yyd, lwd=2, col=gray(.35,.75)) lines(x = xxd, y=yyd1, lwd=2, col=gray(.35,.75), lty=2) lines(x = xxd, y=yyd2, lwd=2, col=gray(.35,.75), lty=2) stripchart(Penguine_Flosse[class==1], method = &quot;jitter&quot;, jitter = .0005, at = .001, pch = 21, col=alpha(col_v[1],.5), bg=alpha(col_v[1],.5), cex=1.3, add = TRUE) stripchart(Penguine_Flosse[class==2], method = &quot;jitter&quot;, jitter = .0005, at = .001, pch = 21, col=alpha(col_v[2],.5), bg=alpha(col_v[2],.5), cex=1.3, add = TRUE) Literatur "],
["1-2-der-em-algorithmus-zur-ml-sch√§tzung-gau√üscher-mischverteilungen.html", "1.2 Der EM Algorithmus zur ML-Sch√§tzung Gau√üscher Mischverteilungen", " 1.2 Der EM Algorithmus zur ML-Sch√§tzung Gau√üscher Mischverteilungen 1.2.1 Gau√üsche Mischmodelle (GMM) Eine Zufallsvariable \\(X\\), die einer Gauschen Mischverteilung folgt, bezeichnen wir als \\[ X\\sim\\mathcal{N}_{\\mathcal{mix}}(G,\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}) \\] Die dazugeh√∂rige Dichtefunktion einer Gau√üschen Mischverteilung ist folgenderma√üen definiert: \\[\\begin{equation} f_G(x|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})=\\sum_{g=1}^G\\pi_gf(x|\\mu_g\\sigma_g) \\tag{1.1} \\end{equation}\\] Gewichte: \\(\\boldsymbol{\\pi}=(\\pi_1,\\dots,\\pi_G)\\) mit \\(\\pi_g&gt;0\\) und \\(\\sum_{g=1}^G\\pi_g=1\\) Mittelwerte: \\(\\boldsymbol{\\mu}=(\\mu_1,\\dots,\\mu_G)\\) mit \\(\\mu_g\\in\\mathbb{R}\\) Standardabweichungen: \\(\\boldsymbol{\\sigma}=(\\sigma_1,\\dots,\\sigma_G)\\) mit \\(\\sigma_g&gt;0\\) Normalverteilung der Gruppe \\(g=1,\\dots,G\\): \\[ f(x|\\mu_g\\sigma_g)=\\frac{1}{\\sqrt{2\\pi}\\sigma_g}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu_g}{\\sigma_g}\\right)^2\\right) \\] Unbekannte Parameter: \\(\\boldsymbol{\\pi}\\), \\(\\boldsymbol{\\mu}\\) und \\(\\boldsymbol{\\sigma}\\) 1.2.2 Maximum Likelihood (ML) Sch√§tzung Man kann versuchen die unbekannten Parameter \\(\\boldsymbol{\\pi}=(\\pi_1,\\dots,\\pi_G)\\), \\(\\boldsymbol{\\mu}=(\\mu_1,\\dots,\\mu_G)\\) und \\(\\boldsymbol{\\sigma}=(\\sigma_1,\\dots,\\sigma_G)\\) eines Gau√üschen Mischmodells klassisch mit Hilfe der Maximum Likelihood Methode zu sch√§tzen. Ich sag‚Äôs gleich: Der Versuch wird scheitern. Wiederholung der Grundidee der ML-Sch√§tzung: Annahme: Die Daten \\(\\mathbf{x}=(x_1,\\dots,x_n)\\) sind eine Realisation einer einfachen (also i.i.d.) Zufallsstichprobe \\((X_1,\\dots,X_n)\\) mit \\[ X_i\\sim\\mathcal{N}_{\\mathcal{mix}}(G,\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}) \\] f√ºr alle \\(i=1,\\dots,n\\). Die Daten \\(\\mathbf{x}=(x_1,\\dots,x_n)\\) ‚Äûkennen‚Äú also die unbekannten Parameter \\(\\boldsymbol{\\pi},\\) \\(\\boldsymbol{\\mu}\\) und \\(\\boldsymbol{\\sigma}\\) und wir m√ºssen ihnen diese Informationen ‚Äûnur noch‚Äú entlocken. Sch√§tz-Idee: W√§hle \\(\\boldsymbol{\\pi}\\), \\(\\boldsymbol{\\mu}\\) und \\(\\boldsymbol{\\sigma}\\) so, dass \\(f_G(\\cdot|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\) ‚Äûoptimal‚Äú zu den beobachteten Daten \\(\\mathbf{x}\\) passt. Umsetzung der Sch√§tz-Idee: Maximiere (bzgl. \\(\\boldsymbol{\\pi}\\), \\(\\boldsymbol{\\mu}\\) und \\(\\boldsymbol{\\sigma}\\)) die Likelihood Funktion \\[\\mathcal{L}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x})=\\prod_{i=1}^nf_G(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\] Bzw. maximiere die Log-Likelihood Funktion (einfachere Maximierung) \\[\\begin{align*} \\ln\\left(\\mathcal{L}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x})\\right)= \\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x}) =&amp;\\sum_{i=1}^n\\ln\\left(f_G(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\right)\\\\ =&amp;\\sum_{i=1}^n\\ln\\left(\\sum_{g=1}^G\\pi_gf(x_i|\\mu_g,\\sigma_g)\\right) \\end{align*}\\] Beachte: Die Maximierung muss die Parameterrestriktionen in (1.1) ber√ºcksichtigen (\\(\\sigma_g&gt;0\\) und \\(\\pi_g&gt;0\\) f√ºr alle \\(g=1,\\dots,G\\) und \\(\\sum_{g=1}^G\\pi_g=1\\)). Die maximierenden Parameterwerte \\(\\hat{\\boldsymbol{\\pi}}\\), \\(\\hat{\\boldsymbol{\\mu}}\\) und \\(\\hat{\\boldsymbol{\\sigma}}\\) sind die ML-Sch√§tzer. Das kann man so ausdr√ºcken: \\[ (\\hat{\\boldsymbol{\\pi}},\\hat{\\boldsymbol{\\mu}},\\hat{\\boldsymbol{\\sigma}})=\\arg\\max_{\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}}\\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x}) \\] üòí Probleme mit Singularit√§ten bei Numerische L√∂sungen: Versucht man obiges Maximierungsproblem numerisch mit Hilfe des Computers zu l√∂sen, wird man schnell merken, dass die Ergebnisse h√∂chst instabil, unplausibel und wenig vertrauensw√ºrdig sind. Der Grund f√ºr diese instabilen Sch√§tzungen sind Probleme mit Singularit√§ten: F√ºr echte GMMs (\\(G&gt;1\\)) treten w√§hrend einer numerischen Maximierung sehr leicht Probleme mit Singularit√§ten auf. Dies geschieht immer dann, wenn eine der Normalverteilungskomponenten versucht den ganzen Datensatz \\(\\mathbf{x}\\) zu beschreiben und die andere(n) Normalverteilungskomponente(n) versuchen lediglich einzelne Datenpunkte zu beschreiben. Eine Gau√üsche Dichtefunktion, die sich um einen einzigen Datenpunkt \\(x_i\\) konzentriert (d.h. \\(\\mu_g=x_i\\) und \\(\\sigma_g\\to 0\\)) wird dabei sehr gro√üe Werte annehmen (d.h. \\(f(x_i|\\mu_g=x_i,\\sigma_g)\\to\\infty\\) f√ºr \\(\\sigma_g\\to 0\\)) und so die Log-Likelihood auf unerw√ºnschte Weise maximieren (siehe Abbildung 1.3). Solch unerw√ºnschte, triviale Maximierungsl√∂sungen f√ºhren i.d.R. zu unplausiblen Sch√§tzergebnissen. Abbildung 1.3: Normalverteilung mit \\(\\mu_g=x_i\\) f√ºr \\(\\sigma_g\\to 0\\). üòí Analytische L√∂sung: Es ist zwar etwas m√ºhsam, aber man kann versuchen die Log-Likelihood analytisch zu maximieren. Tut man dies sich das an, kommt man zu folgenden Ausdr√ºcken: \\[\\begin{align*} \\hat\\pi_g&amp;=\\frac{1}{n}\\sum_{i=1}^np_{ig}\\\\ \\hat\\mu_g&amp;=\\sum_{i=1}^n\\frac{p_{ig}}{\\left(\\sum_{j=1}^np_{jg}\\right)}x_i\\\\ \\hat\\sigma_g&amp;=\\sqrt{\\sum_{i=1}^n\\frac{p_{ig}}{\\left(\\sum_{j=1}^np_{jg}\\right)}\\left(x_i-\\hat\\mu_g\\right)^2} \\end{align*}\\] f√ºr \\(g=1,\\dots,G\\). Die Herleitung der Ausdr√ºcke f√ºr \\(\\mu_g\\), \\(\\sigma_g\\) und \\(\\pi_g\\), \\(g=1,\\dots,G\\), ist wirklich etwas l√§stig (mehrfache Anwendungen der Kettenregel, Produktregel, etc., sowie Anwendung des Lagrange-Multiplikator Verfahrens zur Optimierung unter Nebenbedingungen) aber machbar. In einer der √úbungsaufgaben d√ºrfen Sie den Ausdruck f√ºr \\(\\hat\\mu_g\\) herleiten. üôà Aber: Diese Ausdr√ºcke f√ºr \\(\\hat\\pi_g\\), \\(\\hat\\mu_g\\) und \\(\\hat\\sigma_g\\) h√§ngen von den unbekannten Parametern \\(\\boldsymbol{\\pi}=(\\pi_1,\\dots,\\pi_G)\\), \\(\\boldsymbol{\\mu}=(\\mu_1,\\dots,\\mu_G)\\) und \\(\\boldsymbol{\\sigma}=(\\sigma_1,\\dots,\\sigma_G)\\), denn: \\[ p_{ig}=\\frac{\\pi_gf(x_i|\\mu_g,\\sigma_g)}{f_G(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})} \\] f√ºr \\(i=1,\\dots,n\\) und \\(g=1,\\dots,G\\). Die Ausdr√ºcke f√ºr \\(\\hat\\pi_g\\), \\(\\hat\\mu_g\\), und \\(\\hat\\sigma_g\\) erlauben also keine keine direkte Sch√§tzung der unbekannten Parameter \\(\\pi_g\\), \\(\\mu_g\\) und \\(\\sigma_g\\). ü•≥ L√∂sung: Der EM Algorithmus 1.2.3 Der EM Algorithmus f√ºr GMMs Die Ausdr√ºcke f√ºr \\(\\hat\\pi_g\\), \\(\\hat\\mu_g\\) und \\(\\hat\\sigma_g\\) legen jedoch ein einfaches, iteratives ML-Sch√§tzverfahren nahe: N√§mlich einer alternierenden Sch√§tzung von \\(p_{ig}\\) und \\((\\hat\\pi_g, \\hat\\mu_g,\\hat\\sigma_g)\\). Der Der EM Algorithmus: Setze Startwerte \\(\\boldsymbol{\\pi}^{(0)}\\), \\(\\boldsymbol{\\mu}^{(0)}\\) und \\(\\boldsymbol{\\sigma}^{(0)}\\) F√ºr \\(r=1,2,\\dots\\) (Expectation) Berechne: \\[p_{ig}^{(r)}=\\frac{\\pi_g^{(r-1)}f(x_i|\\mu^{(r-1)}_g,\\sigma_g^{(r-1)})}{f_G(x_i|\\boldsymbol{\\pi}^{(r-1)},\\boldsymbol{\\mu}^{(r-1)},\\boldsymbol{\\sigma}^{(r-1)})}\\] (Maximization) Berechne: \\(\\hat\\pi_g^{(r)}=\\frac{1}{n}\\sum_{i=1}^np_{ig}^{(r)},\\quad\\quad\\hat\\mu_g^{(r)}=\\sum_{i=1}^n\\frac{p_{ig}^{(r)}}{\\left(\\sum_{j=1}^np_{jg}^{(r)}\\right)}x_i\\) \\(\\hat\\sigma_g^{(r)}=\\sqrt{\\sum_{i=1}^n\\frac{p_{ig}^{(r)}}{\\left(\\sum_{j=1}^np_{jg}^{(r)}\\right)}\\left(x_i-\\hat\\mu_g^{(r)}\\right)^2}\\) Pr√ºfe Konvergenz: Stoppe, falls sich der Wert der maximierten Log-Likelihood Funktion, \\(\\ell(\\boldsymbol{\\pi}^{(r)},\\boldsymbol{\\mu}^{(r)},\\boldsymbol{\\sigma}^{(r)}|\\mathbf{z})\\), nicht mehr √§ndert. Der obige pseudo-Code wird im Folgenden Code-Chunck umgesetzt: library(&quot;MASS&quot;) library(&quot;mclust&quot;) ## Daten: x &lt;- cbind(Penguine_Flosse) # Daten [n x d]-Dimensional. d &lt;- ncol(x) # Dimension (d=1: univariat) n &lt;- nrow(x) # Stichprobenumfang G &lt;- 2 # Anzahl Gruppen ## Weitere Deklarationen: llk &lt;- matrix(NA, n, G) p &lt;- matrix(NA, n, G) loglikOld &lt;- 1e07 tol &lt;- 1e-05 it &lt;- 0 check &lt;- TRUE ## EM Algorithmus ## 1. Startwerte f√ºr pi, mu und sigma: pi &lt;- rep(1/G, G) # Naive pi sigma &lt;- array(diag(d), c(d,d,G)) # Varianz = 1 mu &lt;- t(MASS::mvrnorm(G, colMeans(x), sigma[,,1]*4) ) while(check){ ## 2.a Expectation-Schritt for(g in 1:G){ p[,g] &lt;- pi[g] * mclust:::dmvnorm(x, mu[,g], sigma[,,g]) } p &lt;- sweep(p, 1, STATS = rowSums(p), FUN = &quot;/&quot;) ## 2.b Maximization-Schritt par &lt;- mclust::covw(x, p, normalize = FALSE) mu &lt;- par$mean sigma &lt;- par$S pi &lt;- colMeans(p) ## 3. Pr√ºfung der Konvergenz for(g in 1:G) { llk[,g] &lt;- pi[g] * mclust:::dmvnorm(x, mu[,g], sigma[,,g]) } loglik &lt;- sum(log(rowSums(llk))) # aktueller max. Log-Likelihood Wert ## diff &lt;- abs(loglik - loglikOld)/abs(loglik) # √Ñnderungsrate loglikOld &lt;- loglik it &lt;- it + 1 ## √Ñnderungsrate noch gro√ü genug (&gt; tol)? check &lt;- diff &gt; tol } ## Sch√§tz-Resultate: results &lt;- matrix(c(pi, mu, sqrt(sigma)), nrow = 3, ncol = 2, byrow = TRUE, dimnames = list( c(&quot;Gewichte&quot;, &quot;Mittelwerte&quot;, &quot;Standardabweichungen&quot;), c(&quot;Gruppe 1&quot;, &quot;Gruppe 2&quot;))) ## results %&gt;% round(., 2) #&gt; Gruppe 1 Gruppe 2 #&gt; Gewichte 0.31 0.69 #&gt; Mittelwerte 194.27 216.20 #&gt; Standardabweichungen 6.27 7.32 Abbildung 1.4 visualisiert den Fortschritt der iterativen Sch√§tzung mit Hilfe des EM Algorithmuses. Abbildung 1.4: Iterative Sch√§tzung mit Hilfe des EM Algorithmuses. Das finale Sch√§tzergebnis erlaubt es uns, Abbildung 1.2 zu replizieren. "],
["1-3-der-alternative-wahre-blick-auf-den-em-algorithmus.html", "1.3 Der alternative (wahre) Blick auf den EM Algorithmus", " 1.3 Der alternative (wahre) Blick auf den EM Algorithmus Der EM Algorithmus erm√∂glicht es Maximum Likelihood Probleme zu vereinfachen, indem man die Daten durch nicht beobachtete (‚Äûlatente‚Äú) Variablen vervollst√§ndigt. Dieses Prinzip ist der eigentliche wahre Beitrag des EM Algorithmus. Es erm√∂glicht die L√∂sung verschiedener Maximum Likelihood Probleme - wir bleiben aber hier bei der Sch√§tzung von GMMs. Zur Erinnerung: Wir haben es ja nicht geschafft, die Log-Likelihood Funktion \\[ \\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x}) =\\sum_{i=1}^n\\ln\\left(\\sum_{g=1}^G\\pi_gf(x_i|\\mu_g,\\sigma_g)\\right) \\] direkt zu maximieren. Die \\(\\ln(\\sum_{g=1}^G[\\dots])\\)-Konstruktion macht einem hier das Leben schwer. 1.3.1 Vervollst√§ndigung der Daten In unseren Pinguin-Daten gibt zwei Gruppen (\\(g\\in\\{1,2\\}\\)). Es g√§be also im Prinzip \\(G=2\\)-dimensionale Zuordnungsvektoren \\((z_{i1},z_{i2})\\) mit \\[ (z_{i1},z_{i2})= \\left\\{\\begin{array}{ll} (1,0)&amp;\\text{falls Pinguin }i\\text{ zu Gruppe }g=1\\text{ geh√∂rt.}\\\\ (0,1)&amp;\\text{falls Pinguin }i\\text{ zu Gruppe }g=2\\text{ geh√∂rt.}\\\\ \\end{array}\\right. \\] Im Fall von \\(G&gt;2\\) Gruppen: \\[ (z_{i1},\\dots,z_{ig},\\dots,z_{iG})= \\left\\{\\begin{array}{ll} (1,0,\\dots,0)&amp;\\text{falls Datenpunkt }i\\text{ zu Gruppe }g=1\\text{ geh√∂rt.}\\\\ (0,1,\\dots,0)&amp;\\text{falls Datenpunkt }i\\text{ zu Gruppe }g=2\\text{ geh√∂rt.}\\\\ \\quad\\quad\\vdots&amp;\\\\ (0,0,\\dots,1)&amp;\\text{falls Datenpunkt }i\\text{ zu Gruppe }g=G\\text{ geh√∂rt.}\\\\ \\end{array}\\right. \\] Die Zuordnungen \\(z_{ig}\\) k√∂nnen also die Werte \\(z_{ig}\\in\\{0,1\\}\\) annehmen, wobei aber gelten muss, dass \\(\\sum_{g=1}^Gz_{ig}=1\\). Beachte: F√ºr jeden Datenpunkt \\(i\\) (jeder Pinguin \\(i\\)) gibt es nur eine Gruppe (daher \\(\\sum_{g=1}^Gz_{ig}=1\\)). Dies ist eine wichtige Restriktion von GMMs, welche bei den Pinguindaten unproblematisch ist. Bei Anwendungen mit hirarchischen Gruppierungen ist dies aber evtl. problematisch. Die Zuordnungen \\(z_{ig}\\) sind leider unbekannt (latent). Wir wissen aber trotzdem etwas √ºber diese Zuordnungen. Die Gewichte \\(\\pi_1,\\dots,\\pi_G\\) der Gau√üschen Mischverteilung \\[ f_G(x|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})=\\sum_{g=1}^G\\pi_gf(x|\\mu_g\\sigma_g), \\] geben die Anteile der Einzelverteilungen \\(f(\\cdot|\\mu_g\\sigma_g)\\) an der Gesamtverteilung \\(f_G\\) an. Im Schnitt kommen also \\(\\pi_g\\cdot 100\\%\\) der Datenpunkte von Gruppe \\(g\\), \\(g=1,\\dots,G\\). Somit k√∂nnen wir die (latente) Zuordnung \\(z_{ig}\\) als eine Realisation der Zufallsvariablen \\(Z_{ig}\\) mit Wahrscheinlichkeitsfunktion \\[ P(Z_{ig}=1)=\\pi_g \\] auffassen. Wegen der Bedingung \\(\\sum_{g=1}^Gz_{ig}=1\\), gilt dass \\[ Z_{ig}=1\\quad \\Rightarrow\\quad Z_{i1}=0,\\dots,Z_{ig-1}=0,Z_{ig+1}=0,\\dots,Z_{iG}=0. \\] 1.3.2 A-priori und A-posteriori Wahrscheinlichkeiten: \\(\\pi_g\\) und \\(p_{ig}\\) A-priori-Wahrscheinlichkeit \\(\\pi_g\\): Man bezeichnet die Wahrscheinlichkeiten \\(\\pi_g\\) als die a-priori-Wahrscheinlichkeiten. Wenn wir nichts √ºber die Flossenl√§nge von Pinguin \\(i\\) wissen, dann bleiben uns nur die a-priori-Wahrscheinlichkeiten: ‚ÄûMit Wahrscheinlichkeit \\(\\pi_g=P(Z_{ig}=1)\\) geh√∂rt Pinguin \\(i\\) zu Gruppe \\(g\\).‚Äú A-posteriori-Wahrscheinlichkeit \\(\\;p_{ig}\\): Falls wir die Flossenl√§nge \\(x_i\\) von Pinguin \\(i\\) erfahren, k√∂nnen wir die a-priori-Wahrscheinlichkeiten mit Hilfe des Satzes von Bayes aktualisieren. Dies f√ºhrt dann zur a-posteriori-Wahrscheinlichkeit: ‚ÄûMit Wahrscheinlichkeit \\(p_{ig}=P(Z_{ig=1}|X_i=x_i)\\) geh√∂rt ein Pinguin \\(i\\) mit Flossenl√§nge \\(x_i\\) zu Gruppe \\(g\\). Satz von Bayes: \\[\\begin{align*} p_{ig} &amp;=\\frac{\\pi_gf(x_i|\\mu_g,\\sigma_g)}{f_G(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}\\\\[2ex] &amp;=\\frac{\\overbrace{P(Z_{ig}=1)}^{\\text{‚ÄûA-priori-Wahrs.‚Äú}}f(x_i|\\mu_g,\\sigma_g)}{f_G(x_i|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}=\\overbrace{P(Z_{ig}=1|X_i=x_i)}^{\\text{‚ÄûA-posteriori-Wahrs.‚Äú}}=p_{ig}\\\\ \\end{align*}\\] 1.3.3 Der (bedingte) Mittelwert: \\(p_{ig}\\) Beachte: Die a-posteriori-Wahrscheinlichkeiten \\(p_{ig}\\) sind tats√§chlich (bedingte) Erwartungswerte: \\[\\begin{align*} p_{ig}&amp;= 1\\cdot P(Z_{ig}=1|X_i=x_i)+0\\cdot P(Z_{ig}=0|X_i=x_i) = E(Z_{ig}|X_i=x_i)\\\\ \\end{align*}\\] Damit ist die Berechnung von \\(p_{ig}\\) im (Expectation)-Schritt des EM Algorithmuses (siehe Kapitel 1.2.3) also tats√§chlich eine Erwartungswertberechnung. "],
["1-4-das-gro√üe-ganze.html", "1.4 Das Gro√üe Ganze", " 1.4 Das Gro√üe Ganze H√§tten wir neben den Datenpunkten \\(\\mathbf{x}=(x_1,\\dots,x_n)\\) auch die Gruppenzuordnungen \\(\\mathbf{z}=(z_{11},\\dots,z_{nG})\\) beobachtet, dann k√∂nnten wir folgende Likelihood (\\(\\tilde{\\mathcal{L}}\\)) bzw. Log-Likelihood (\\(\\tilde{\\ell}\\)) Funktion aufstellen: \\[\\begin{align*} \\tilde{\\mathcal{L}}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x},\\mathbf{z}) &amp;=\\prod_{i=1}^n\\prod_{g=1}^G\\left(\\pi_gf(x_i|\\mu_g,\\sigma_g)\\right)^{z_{ig}}\\\\[2ex] \\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x},\\mathbf{z}) &amp;=\\sum_{i=1}^n\\sum_{g=1}^Gz_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(f(x_i|\\mu_g,\\sigma_g)\\right)\\right\\} \\end{align*}\\] Im Gegensatz zur urspr√ºnglichen Log-Likelihood Funktion (\\(\\ell\\)), w√§re die neue Log-Likelihood Funktion \\(\\tilde\\ell\\) einfach zu maximieren, da hier keine Summe innerhalb der Logarithmusfunktion steckt, sodass wir direkt den Logarithmus der Normalverteilung berechnen k√∂nnen. Dies vereinfacht das Maximierungsproblem deutlich, da die Normalverteilung zur Exponentialfamilie geh√∂rt. Aber: Wir beobachten die Realisationen \\(\\mathbf{z}=(z_{11},\\dots,z_{nG})\\) nicht, sondern kennen lediglich die Verteilung der Zufallsvariablen \\(\\mathbf{Z}=(Z_{11},\\dots,Z_{nG})\\). Dies f√ºhrt zu einer stochastischen Version der Log-Likelihood Funktion: \\[ \\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x},\\mathbf{Z})=\\sum_{i=1}^n\\sum_{g=1}^GZ_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(f(x_i|\\mu_g,\\sigma_g)\\right)\\right\\} \\] Von dieser k√∂nnen jedoch den bedingten Erwartungswert berechnen: \\[ E(\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x},\\mathbf{z})|X_i=x_i)=\\sum_{i=1}^n\\sum_{g=1}^Gp_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(f(x_i|\\mu_g,\\sigma_g)\\right)\\right\\} \\] 1.4.1 Der EM Algorithmus: Die abstrakte Version Der folgende EM Algorithmus unterscheidet sich wieder lediglich in der Notation von den oben bereits besprochenen Versionen (siehe Kapitel 1.2.3). Die hier gew√§hlte Notation verdeutlicht, dass der Expectation-Schritt die zu maximierende Log-Likelihood Funktion aktualisiert und diese dann im (Maximization)-Schritt maximiert wird. Dar√ºber hinaus ist die gew√§hlte Notation abstrakt genug, um die Grundidee des EM Algorithmuses auf andere Maximum Likelihood Probleme zu √ºbertragen. Im Folgenden wird der Parametervektor \\((\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\) der Einfachheit halber auch mit \\(\\boldsymbol{\\theta}\\) bezeichnet. Setze Startwerte \\(\\boldsymbol{\\theta}^{(0)}=(\\pi^{(0)}, \\mu^{(0)}, \\sigma^{(0)})\\) F√ºr \\(r=1,2,\\dots\\) (Expectation) Berechne: \\[\\begin{align*} \\mathcal{Q}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{(r-1)}) &amp;=E_{\\boldsymbol{\\theta}^{(r-1)}}(\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}|\\mathbf{x},\\mathbf{z})|X_i=x_i)\\\\ &amp;=\\sum_{i=1}^n\\sum_{k=1}^Kp_{ig}^{(r-1)}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(f(x_i|\\mu_g,\\sigma_g)\\right)\\right\\} \\end{align*}\\] (Maximization) Berechne: \\[\\begin{align*} \\boldsymbol{\\theta}^{(r)}=\\arg\\max_{\\boldsymbol{\\theta}}\\mathcal{Q}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{(r-1)}) \\end{align*}\\] Pr√ºfe Konvergenz: Stoppe, falls sich der Wert der maximierten Log-Likelihood Funktion, \\(\\mathcal{Q}(\\boldsymbol{\\theta}^{(r)},\\boldsymbol{\\theta}^{(r-1)})\\), nicht mehr √§ndert. Ende Dem gemeinen Pinguin ist der EM Algorithmus egal (Abbildung 1.5). Abbildung 1.5: Pinguinforschung am Limit. "],
["literatur.html", "Literatur", " Literatur "]
]
